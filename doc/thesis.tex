%-----------------------------------------------
%  Engineer's & Master's Thesis Template
%  Copyleft by Artur M. Brodzki & Piotr Woźniak
%  Warsaw University of Technology, 2019-2022
%-----------------------------------------------

\documentclass[
    bindingoffset=5mm,  % Binding offset
    footnoteindent=3mm, % Footnote indent
    hyphenation=true    % Hyphenation turn on/off
]{src/wut-thesis}

%\usepackage{setspace}
%\onehalfspacing
%\frenchspacing

\graphicspath{{tex/img/}} % Katalog z obrazkami.

%\usepackage[style=authoryear]{biblatex}
\addbibresource{bibliografia.bib} % Plik .bib z bibliografią

% Do debugowania czy tekst wychodzi poza margines
%\usepackage{showframe}

\usepackage{booktabs} % for better table lines
\usepackage{minted}
\usepackage{mdframed}
\usepackage{float}
\newtheorem{example}{Example}
\usepackage{pifont}   % for ding symbols
\usepackage{svg}

\newcommand{\cmark}{\ding{51}} % checkmark
\newcommand{\xmark}{\ding{55}} % cross

\mdfdefinestyle{mintedframe}{
    innertopmargin=1mm,
    frametitlebelowskip=0pt,
    frametitleaboveskip=0pt,
    splittopskip=0pt,
    linewidth=0.75pt
}
\surroundwithmdframed[style=mintedframe]{minted}

\mdfdefinestyle{verbatimframe}{
    innertopmargin=1mm,
    frametitlebelowskip=0pt,
    frametitleaboveskip=0pt,
    splittopskip=0pt,
    linewidth=0.75pt
}
\surroundwithmdframed[style=verbatimframe]{verbatim}

%-------------------------------------------------------------
% Wybór wydziału:
%  \facultyeiti: Wydział Elektroniki i Technik Informacyjnych
%  \facultymeil: Wydział Mechaniczny Energetyki i Lotnictwa
% --
% Rodzaj pracy: \EngineerThesis, \MasterThesis
% --
% Wybór języka: \langpol, \langeng
%-------------------------------------------------------------
\facultyeiti    % Wydział Elektroniki i Technik Informacyjnych
\MasterThesis % Praca inżynierska
\langeng % Praca w języku polskim

\begin{document}

%------------------
% Strona tytułowa
%------------------
\instytut{Control and Computation Engineering}
\kierunek{computer science}
\specjalnosc{Internet Management Support Systems}
\title{
    Machine learning framework for explainable artificial intelligence models
}
% Title in English for English theses
% In English theses, you may remove this command
\engtitle{
    Machine learning framework for explainable artificial intelligence models
}
% Title in Polish for English theses
% Use it only in English theses
\poltitle{
    Framework uczenia maszynowego dla modeli audio wyjaśnialnej sztucznej inteligencji
}
\author{Maciej Falkowski, BSc}
\album{329117}
\promotor{Mateusz Modrzejewski, PhD}
\date{\the\year}
\maketitle

%-------------------------------------
% Streszczenie po polsku dla \langpol
% English abstract if \langeng is set
%-------------------------------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\abstract

The aim of this thesis is to design a tool that enables the execution, visualization, and comparison of
Explainable Artificial Intelligence (XAI) methods for audio machine learning models. As part of this
the thesis describes the design of an audio XAI framework, which allows its user to integrate their models
with the framework's API in the form of a library, which then performs explanations on the model and visualizes
them in a web interface designed for the thesis. The thesis describes the developed framework architecture
based on the Model-View-Presenter (MVP) pattern, which aims to ensure the solution's modularity, enabling
integration of new user models and alternative views to the default one. The project integrates selected
XAI methods, including Local Interpretable Model agnostic Explanations (LIME), Integrated gradients,
and Layer-wise Relevance Propagation (LRP).

\keywords Explainable artificial intelligence, Framework, Interpretability, Explainability, Model-View-Presenter, Integrated gradients, Layer-wise Relevance Propagation

%----------------------------------------
% Streszczenie po angielsku dla \langpol
% Polish abstract if \langeng is set
%----------------------------------------
\clearpage
\secondabstract

Celem pracy magisterskiej jest zaprojektowanie narzędzia umożliwiającego wykonanie,
wizualizację oraz zestawienie metod Wyjaśnialnej sztucznej inteligencji (XAI) dla modeli uczenia maszynowego audio.
W ramach tego praca opisuje projekt frameworka wyjaśnień XAI modeli audio (audio XAI framework),
który umożliwia użytkownikowi integrację jego modeli z API frameworka w postaci biblioteki,
która następnie wykonuje wyjaśnienia na modelu, a następnie wizualizuje je w interfejsie
webowym, który został zaprojektowany na potrzeby pracy. Praca opisuje opracowaną architekturę frameworku
opartą o wzorzec Model-View-Presenter (MVP), której celem jest zapewnienie modularności rozwiązania, umożliwiającej
integrację nowych modeli użytkownika oraz alternatywnych widoków względem domyślnego.
Projekt integruje wyselekcjonowane metody XAI wliczające w nie Local Interpretable Model-agnostic Explanations (LIME),
Integrated gradients oraz Layer-wise Relevance Propagation (LRP).


\secondkeywords Explainable artificial intelligence, Framework, Interpretability, Explainability, Model-View-Presenter, Integrated gradients, Layer-wise Relevance Propagation

\pagestyle{plain}

%--------------
% Spis treści
%--------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\tableofcontents

%------------
% Rozdziały
%------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\pagestyle{headings}

% #############################################
%
% Rozdział 1 - Introduction
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Introduction} \label{ch:introduction}

Artificial Intelligence (AI) continues to grow rapidly, containing a broad range of applications,
including audio processing. The increasing complexity of AI systems brings the challenge of understanding
and interpreting their decision-making processes. Many state-of-the-art models, such as deep
neural networks (DNNs) especially Convolutional neural networks \cite{Lecun1998},
frequently used for audio analysis, operate as so-called \emph{black boxes}. While these models deliver high accuracy,
their inner workings often remain opaque to human observers, limiting trust, interpretability,
and the ability to validate results.

In this situation, Explainable Artificial Intelligence (XAI) has emerged as a promising and quickly expanding area
of research \cite{Arrieta2019-yr}.
XAI aims to make the decision-making process of complex AI systems more transparent to allow developers
and researchers to better understand why a model produces a given output. There are many different explanation
techniques, each operating on distinct principles and offering a unique standpoint
on a model's decision-making process \cite{Abhishek2022-iu}.
As a result, it is often necessary to apply multiple explanation methods to the same input and then compare
and contrast their results. For audio models, it is additionally valuable to visualize supplementary audio elements,
such as audio playback or spectrogram representations, to provide richer context for analysis. This necessity,
however, leads to repetitive and time-consuming workflows for the user, which must be repeated for each
explanation method used. Developing a tool that allows the user to execute multiple explanations on their
model and then visualize these results in an integrated manner would greatly reduce the amount of
manual work required.

The thesis aims to design a tool that enables the execution, visualization, and comparison of XAI methods for
audio machine learning models. As part of this goal, the thesis describes the design of an audio-focused XAI
framework as the proposed solution to the identified problem.

The framework provides an Application Programming Interface (API) in the form of a Python library
and supporting tools to allows its users to integrate their own models into the system and run various XAI techniques.
The framework's architecture is based on the Model-View-Presenter (MVP) pattern to provide modularity and ensure
integration of additional models and alternative presentation form to the web interface. The framework
integrates selected XAI methods, including Local Interpretable Model-agnostic Explanations (LIME), Integrated
Gradients, and Layer-wise Relevance Propagation (LRP). The visualization layer is implemented as a web interface
based on the JavaScript React library, which enables presentation of results from multiple explanation methods
alongside audio playback and spectrogram visualization. The scope of the thesis is not to perform any research about
integrated machine learning models and explanation techniques. The thesis does not evaluate any of their metrics such
as performance or accuracy, nor it compares them one to another.

The content of the thesis has been split into individual chapters. Chapter \ref{ch2:probAnalysis} discusses
the fundamentals of Explainable artificial intelligence, provides the analysis for the selected methodology for
the project which is a framework architecture, and discusses the selection of XAI methods and models integrated
into the project. Chapter \ref{ch:reqrTools} describes the functional and non-functional requirements for each
component of the framework as well as software requirements required for the implementation of the project of
the thesis. Chapter \ref{ch:externalSpec} provides software requirements and the user manual of the project
together with an overview of the user interface. Chapter \ref{ch:implementation} describes the internal components of
the project, describing in detail the implementation of each step of interpretation. Chapter \ref{ch:verification}
describes the testing and validation procedure of the project. Chapter \ref{ch:summary} provides a summary
of the results obtained through project development, draws conclusions about them, and outlines ideas for
a future research about similar topics.

% #############################################
%
% Rozdział 2 - Analiza problemu
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Problem analysis} \label{ch2:probAnalysis}

This chapter provides an overview of terminology and methods used in the thesis,
which describes essentials of Explainable Artificial Intelligence, description of XAI methods
used in the thesis, and software engineering principles
used for designing an Audio XAI framework.

\subsection{Black-box and white-box AI systems}

The AI (Artificial Intelligence) systems may be classified as \emph{black-box} or \emph{white-box} models
based on their interpretability characteristic \cite{Castelvecchi2016}. This classification is performed based on the
transparency characteristic of the neural network, which is defined as a level of understanding of an internal behavior
and decision-making process by a human observer \cite{Arrieta2019-yr}.

White-box models have high-level transparency, and a human may understand their internal workings.
The examples of such models include decision trees or linear
regression models. In contrast, black-box models lack transparency, and their internals are opaque. The examples
of black-box models are Convolutional Neural Networks (CNNs) or Deep Neural Networks in general. One of the methods
to introspect the internals of black-box models is post-hoc explanation methods (\ref{ch2:Xai}) that allow
introspection after the model processing, as shown in Fig. \ref{fig:BlackBoxModel}.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering`
    \includegraphics[width=14cm]{Black-box-post-hoc.png}
    \caption{The diagram depicts a black-box model with post-hoc explanation techniques applied to its output.}
    \label{fig:BlackBoxModel}
\end{figure}

\subsection{Explainable artificial intelligence} \label{ch2:Xai}

Explainable Artificial Intelligence (XAI) is a collection of machine learning
techniques that focuses on making the decision-making process
of AI (Artificial Intelligence) systems understandable to humans \cite{Arrieta2019-yr}.
XAI seeks to provide \emph{explainability} of a model, which is a characteristic
that describes any action or procedure that provides \emph{interpretability} of its
internal decision-making process. The other XAI characteristic is \emph{interpretability} of the model, which is
the ability of the model to provide a result understandable by a human. 

The main goals of Explainable artificial intelligence are to increase transparency, understandability,
and trustworthiness of AI systems for humans. The other advantages of XAI are for developers of AI systems
to assist them with debugging and improving their systems.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=\linewidth]{XAI-Taxonomy.png}
    \caption{The taxonomy of the Explainable Artificial Intelligence \cite{Patricio2022}.}
    \label{fig:XaiTaxonomy}
\end{figure}
 
The wide number of XAI techniques may be characterized into multiple categories forming
a XAI taxonomy (Fig. \ref{fig:XaiTaxonomy}), including:
\begin{itemize} [itemsep=1\baselineskip]
    \item \textbf{Model-Specific vs. Model-Agnostic} - The model-agnostic methods can be applied to
        any model without taking into account its internal structure. On the other hand, model-specific
        techniques are tailored towards a particular type of model, for example, Convolutional Neural Networks,
    \item \textbf{Locality of Explanation} - Local interpretability aims at explaining a single prediction
        or instance to help understand why the model made a given decision. An example may be a LIME technique,
        which may show which audio feature or part influenced a given prediction. In contrast, global
        interpretability aims to explain the whole behavior of the model across the entire dataset,
    \item \textbf{Model opaqueness} - This criterion characterizes XAI techniques based on the transparency
        of models, which are black-box and white-box (section). White-box models are intrinsic, which means that
        they are naturally explainable, while black-box models have a complex internal structure, which means
        that only post-hoc methods can be used, i.e., those that explain the behavior of the model
        without explaining its internal implementation.
    \item \textbf{Modality} - It classifies the techniques based on the type or form of output
        in which the explanation is presented to the user. The exemplary types of modality
        are textual or visual.
\end{itemize}

\subsection{Selection of explanation methods for framework}

The thesis focuses on post-hoc explanation methods and provides a set of XAI methods integrated
into the framework. The essential criteria for selecting these explanation methods were the
interpretability of the produced explanation, simplicity of the user interface of a given XAI
technique, whether it depends on a background data sample, and whether the method is model-specific
or agnostic. The following XAI methods were considered as methods integrated into the framework:
\begin{itemize}
    \item \textbf{Local Interpretable Model Explanation (LIME)} - The LIME is an XAI technique that
        provides local, model-agnostic explanations. The LIME implementation used for the project
        is \emph{audioLIME}, which utilizes a source separation technique to provide \emph{listenable}
        explanations that are interpretable for the user (Sec. \ref{ch2:LimeMethod}),
    \item \textbf{Integrated Gradients} - Integrated Gradients is an attribution-based XAI technique
        that fulfills axiomatic requirements of sensitivity and implementation invariance, making it
        very reliable \cite{Sundararajan2017-zy} (Sec. \ref{ch2:IgradMethod}),
    \item \textbf{Layer-wise Relevance Propagation (LRP)} - The LRP is a gradient-based technique that
        with a similar interface to the integrated gradients which is an attribution map
        (Sec. \ref{ch2:LrpMethod}),
    \item \textbf{SHapley Additive exPlanations (SHAP)} - The SHAP explanation method \cite{NIPS2017_8a20a862}
        was considered for the framework. It was rejected as it requires a background set of data
        samples, making it data-dependent \cite{Spinner2019-jg}, which complicates the
        implementation of that method for the project.
\end{itemize}

The XAI techniques that were selected for the method integrated into the framework
are LIME, Integrated Gradients, and Layer-wise relevance propagation.

\subsection{Local Interpretable Model Explanation} \label{ch2:LimeMethod} % AudioLIME

The Local Interpretable Model Explanation (LIME) is an Explainable AI technique that
makes a post-hoc prediction of a given model locally and model-agnostic,
by fitting a small model around the prediction \cite{Ribeiro2016-cw}.
The method perturbs interpretable components
by creating a surrogate model $g \in G$ that finds the interpretable features
around the input value to determine which interpretable features impact the model's
decision.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=12cm]{LIME-Diagram.png}
    \caption{The diagram depicts the process of the LIME explanation \cite{Knab2025-pz}.}
    \label{fig:LimeSteps}
\end{figure}

For the whole LIME procedure (Fig. \ref{fig:LimeSteps}), the first step is generating samples
where LIME creates a set of perturbed samples to locally approximate a
given explained model $f$ for a given input $x \in X$.
LIME defines a set of interpretable inputs $x'$, which are perturbed samples of the input
and represented as a set of binary vectors $z$, and the mapping $x =h_x(x')$, which converts
the vectors back to the original input space. Different mapping $h_x$ are used depending
on the type of the input space, like input or text data.
The LIME explanation is calculated by minimizing a loss function $\mathcal{L}(f,g,\pi_x)$:
\begin{equation}
\xi(x) = \arg\min_{\substack{g \in G}} \mathcal{L}(f, g, \pi_{x}) + \Omega(g).
\end{equation}

where $\pi_x$ is defined as a proximity measure between a given input $x$ and a given
binary vector $z$. The $\Omega$ is a complexity measure.

\subsection{audioLIME} \label{ch2:audioLIME}
AudioLIME is an XAI technique based on the LIME framework that extends its definition to audio data.
The method applies source separation to the input audio waveform, which separates sound sources such
as voice or musical instruments from the original audio input \cite{Haunschmid2020-dd}.
The audioLIME method closely follows a pipeline of the LIME method but applies source
separation to the input, as shown in Fig. \ref{fig:audioLIME-Diagram}.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=13cm]{audioLIME-Diagram.png}
    \caption{The diagram shows the pipeline of the audioLIME method.}
    \label{fig:audioLIME-Diagram}
\end{figure}

For a given input, the method defines a set of interpretable features - audio segments in this scenario - and
generates a set of binary vectors by performing perturbation. The surrogate model then trains on these
samples originating from the decomposed input, split into multiple audio sources by segmentation,
and then perturbed.

The method generates listenable explanations that
provide interpretability to the user. Figure \ref{fig:audio-lime-wyjaśnienie} shows an example of
audioLIME’s explanation, where the result contains an audio segment with the highest contribution
to the model’s prediction.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=13cm]{audio-lime-wyjaśnienie.png}
    \caption{The image shows the result of audioLIME's explanation.}
    \label{fig:audio-lime-wyjaśnienie}
\end{figure}

\subsection{Integrated gradients} \label{ch2:IgradMethod}

Integrated gradients (IG) is an attribution-based XAI technique that provides an explanation
of the explained model by accumulating gradients from the baseline input $x'$ to the input $x$
along a path integral of the gradients between those two inputs \cite{Sundararajan2017-zy}.
The definition is specified for a given input model $F : R^n \rightarrow [0,1]$,
the input $x \in R^n$, and the baseline input $x' \in R^n$.

The integrated gradients in the $i^{th}$ dimension is defined for
a gradient $\frac{\partial F\left(x\right)}{\partial x_i}$ as:
\[
\text{IG}_i(x) = (x_i - x'_i) \times \int_{\alpha=0}^{1} \frac{\partial F\left(x' + \alpha(x - x')\right)}{\partial x_i} \, d\alpha.
\]

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=13cm]{Attribution-map.jpg}
    \caption{The image the attribution map of Integrated Gradient's explanation.}
    \label{fig:IgAttributionMap}
\end{figure}

The output of the LRP method is an attribution map as shown in Fig. \ref{fig:IgAttributionMap}.

\subsection{Layer-wise Relevance Propagation} \label{ch2:LrpMethod}
Layer-wise Relevance Propagation (LRP) is a gradient-based XAI method that
explains a given classifier’s decision using decomposition and relies on a backward propagation
mechanism that is applied to all the model’s layers in the sequence \cite{Binder2016-oa}.

A given prediction $f(x)$ is redistributed backwards using a given
redistribution rule until a given relevance score $R_i$, like an image fragment or pixel,
is calculated and assigned, ensuring that \emph{relevance conservation} is preserved such that:
\[
\sum_{i} R_{i} = \ldots = \sum_{j} R_{j} = \sum_{k} R_{k} = \ldots = f(\mathbf{x}) \tag{1}
\]

Relevance conservation states that the total value of relevance is preserved
at every step, such as every layer of the neural network, of the redistribution process.
The relevance score $R_i$ of each input variable informs how strongly this variable contributed
to the prediction. 

The redistribution process from layer  $l + 1$ to layer $l$ can be
defined for feed-forward neural networks as follows:
\[
R_j = \sum_k \frac{x_j w_{jk}}{\sum_{j} x_{j} w_{jk} + \varepsilon} R_k.
\]
for the relevance score $R_k$ at a layer $l + 1$, the weight connection $w_{jk}$ between
neuron $j$ and neuron $k$, and $x_j$ neuron activations at a layer $l$.

Different LRP redistribution rules, such as \( \varepsilon \)-rule or \( \gamma \)-rule,
are defined for a given layer type.
The output of the LRP method is an attribution map, similar to Integrated gradients,
as shown in Fig. \ref{fig:IgAttributionMap}.

\subsection{Sound waveform representation}
The sound wave may be represented using a \emph{waveform}, which is a graphical
representation of a function of a sound in the time domain as depicted in
Fig. \ref{fig:waveform-sampling} \cite{Anand2024}.
The digital representation of a sound wave is performed by sampling the analog waveform,
usually at a regular period every $T$ seconds, which is a \textbf{sampling interval}. The measure that provides
a value of average samples per second is the \textbf{sample rate} $f$, which is calculated as $f = \frac{1}{T}$.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=12cm]{waveform-sampling.png}
    \caption{The process of sampling a sine audio waveform with constant interval.
        Sampled points are represented using red dots.}
    \label{fig:waveform-sampling}
\end{figure}

The sound may be represented in a graphical form in a time-frequency domain using a \emph{spectrogram}.
The spectrogram is a spectrum of frequencies of a sound at a given time point in the time domain,
where the color at a given point is the amplitude of a given wave \cite{Zhang2019AudioRU}.
This representation of audio is important in Machine Learning as it allows us to represent audio as image
data and use different types of neural networks, such as Convolutional neural networks. Sound may be 
represented in the \textbf{Mel scale}, which is a non-linear, perceptual scale to better reflect the subjective
reception of a sound level to a human rather than a standard sound scale presented in Hertz \cite{Volkmann2005}.
The \textbf{mel-spectrogram} is a spectrogram whose frequency scale was transformed into the mel scale.
An example of a mel-spectrogram is shown in Fig. \ref{fig:SpectrogramImage}.
The mel-spectrogram is frequently used in the domain of machine learning as its format in the form
of an image accurately approximates how humans perceive sound \cite{Zhang2019AudioRU}.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=12cm]{spectrogram.png}
    \caption{The picture shows an exemplary spectrogram.}
    \label{fig:SpectrogramImage}
\end{figure}

\subsection{Selection of ML model for integration} \label{ch2:ModelSelection}

The audio XAI framework contains several machine learning models that were prematurely integrated
into the framework as part of the thesis. The selection of these models focused on
selecting state-of-the-art audio classifier models.

The models integrated into the framework as a part of the project include:
\begin{itemize}
    \item \textbf{HarmonicCNN} -  The HarmonicCNN model from the repository
        \textit{State-of-the-art Music Tagging Models},
        which is a collection of audio-tagging models \cite{Won2020-ej}. The model is a
        novel approach to audio classification by using harmonic filters \cite{MinzWon2020},
    \item \textbf{CNN14} - The state-of-the-art music-tagging CNN model from the \emph{Paans} collection
        of models trained on the \emph{AudioSet} dataset \cite{AudioSet2017} that outperformed previous
        state-of-the-art Google’s models in mean average precision (mAP) \cite{Kong2019-wu},  
    \item \textbf{GtzanCNN} - The GtzanCNN is a handmade CNN model for the thesis based on the GTZAN dataset
        \cite{Sturm2013-ma}. It was specifically designed to take a spectrogram directly as an input,
        in contrast to the other models, such as CNN14 and HarmonicCNN, which convert a waveform to
        the spectrogram in one of their layers. As later shown (Sec. \ref{ch6:ExplanationSupport}), this
        allowed the model to run the LRP method as all its layers were supported by the LRP implementation
        of the \emph{captum} library.
\end{itemize}

\subsection{Dependency Inversion Principle} \label{ch2:DependencyInversion}

Dependency Inversion Principle (DIP) is a software engineering method stemming from
the SOLID principle that provides a methodology for decoupling software modules.
The DIP principle states in its definition \cite{martin2003agile}:

\begin{definition}
High-level modules should not depend on low-level modules. Both should depend on abstractions.
Abstractions should not depend on details. Details should depend on abstractions.
\end{definition}

The example of the Dependency Principle Inversion is depicted in Fig. \ref{fig:DepInversionPrinciple}.
The first high-level class \texttt{Controller} directly uses a given machine learning model by
creating its instance directly by calling \mintinline{py}{MLModel()} in its constructor.
This creates a dependency of a high-level class on the implementation of the low-level one.
The second class \texttt{ControllerDecoupled} has an intermediate interface \texttt{PredictorInterface},
which abstracts away the details of the low-level \texttt{MLModel} class and provides a clear interface for other
low-level classes and the high-level class. This approach opens a high-level class for extension - any other
class implementing the interface can be passed to the interface. The principle allows to make
systems more maintainable, modular, and testable.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class Controller:
    def __init__(self):
        self.model = MLModel()  # tightly coupled

class ControllerDecoupled:
    def __init__(self, predictor: PredictorInterface):
        self.model = predictor  # decoupled via abstraction
\end{minted}
\caption{The example of the DIP principle implemented in the Python programming language.}
\label{fig:DepInversionPrinciple}
\end{figure}

\subsection{Software framework} \label{ch2:SoftwareFramework}

A software framework is a software component that provides a generic, structured functionality
for creating applications. It provides predefined components, tools, and API (Application Programming Interface)
so that users may focus on implementing their programs \cite{buschmann1996pattern}.
It differs from a \emph{library} by applying an Inversion of Control so that, rather than
providing a certain functionality for a user to call, it provides a certain rigid program
structure, calling the user’s provided code at some points.

The software framework has key characteristics:
\begin{itemize}
    \item \textbf{Inversion of Control} - A key characteristic where a framework establishes a predefined
        software structure and invokes user-provided components at specific points. This inversion of
        control distinguishes a framework from a library. This principle allows modularity,
        separation of concerns, and reduces boilerplate code,
    \item \textbf{Default structure} - A framework typically has a consistent architecture, and its
        core code is generally fixed. It may be structured using a specific design pattern like
        Model-View-Controller to ensure modularity of the implementation. The framework may improve
        the organization of a code by providing a predefined structure,
    \item \textbf{Extensibility} - Allows for extension and customization of framework behavior.
        It may be offered through different solutions such as plugin systems, hooks, or abstract
        classes that developers can implement.
\end{itemize}

\subsection{Model-View-Presenter} \label{ch2:ModelViewController}

Model-View-Presenter (MVP) \cite{Potel2011} is an architectural design pattern that is a variation
of the MVC (Model-View-Controller) that aims to provide a separation of concerns between
data processing (business logic) and user interface.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    %\includegraphics[width=5cm]{MVP-Pattern.png}
    \includesvg[width=6cm]{MVP-Pattern.svg}
    \caption{The diagram depicts Model-View-Presenter pattern organization.}
    \label{fig:MvpDiagram}
\end{figure}

The MVP pattern separates a program’s logic into the three components, which are
\emph{Model}, \emph{View}, and \emph{Presenter}. The organization of the relation
between components is shown in Fig. \ref{fig:MvpDiagram}.

\begin{description}
    \item[Model] The model is responsible for information processing and
        generating a data to be displayed,
    \item[View] It is a \textbf{passive} component that is responsible for presenting 
        generated data results to the user. The key distinction of the MVP pattern
        to the classic MVC pattern is that the view does not communicate with a
        model in any way and rather it is immutable. The example of such a view
        could be a static web page,
    \item[Presenter] The component that links the other two. Presenter retrieves model's
        results and formats them for the view.
        It manages user's input and commands and passes them to the model and view appropriately.
\end{description}

A key aspect identified when designing an Audio XAI framework was that data processing needs
to be performed for multiple user-supplied models, and their results are subsequently visualized.
This situation allowed adaptation of the MVP pattern to ensure modularity of the solution,
bringing a necessary separation of concerns to the solution so that data processing and
visualization are independent of each other, controlled through a presenter class.
The Audio XAI framework implemented in the thesis generates a static web page which
distinguish its MVP implementation from a typical one as view does not communicate completely
with a presenter class.

\subsection{Audio XAI Framework architecture} \label{ch2:ArchitectureOverview}

The proposed architecture for the topic of the thesis that allows generating explanations for ML models,
along with visualizing them, is the \emph{framework} architecture (\ref{ch2:SoftwareFramework}).
This decision originates from the observation that a wide range of machine learning models are
defined using various libraries and user APIs. To make a project usable, there must be a minimal
amount of code created to adapt a given external model for use in the project.
Consequently, the intended user of such a tool is a ML model designer or user, including programmers,
data scientists, etc.

The other application architecture that was considered was a monolithic user
application. This design was rejected in contrast to the framework as the project
requires a minimal influence of the user to adapt their models,
meaning that some form of inversion of control (Sec. \ref{ch2:SoftwareFramework}) is required, which
naturally suggests the framework architecture.
The framework architecture is shown in Fig.\ref{fig:FrameworkArch}.

Furthermore, the framework is based on the MVP pattern (Sec. \ref{ch2:ModelViewController})
to provide a modular and extensible implementation. This allows a framework to be extended
with a new ML model, XAI technique, or presentation layer with no or minimal interaction
with the rest of the implementation.

\begin{figure}%[h!] % TODO: h! musi być wyłączone
    \centering`
    \includegraphics[width=12cm]{Framework-arch.png}
    \caption{The Audio XAI framework architecture.}
    \label{fig:FrameworkArch}
\end{figure}

The framework is separated logically into the following parts, including:
\begin{description}
    \item[Library] Provides an Application programming interface for the user
        of the library that allows creation of an adapter for a user's model,
        setup of a given explanation method, its configuration
        including a selection of a given presentation layer, and utilities for processing and audio,
    \item[Web interface] It is a presentation layer of the framework that allows visualization
        of the results of XAI techniques, the audio data including its playback and spectrogram
        visualization,
    \item[Runner script] The runner script is an utility script provided by the framework
        to help the user to launch explanation's on libraries supported models (\ref{ch2:ModelSelection})
        in a typical usage scenario where the input audio is read and preprocessed for
        a given model and then the explanation is launched.
\end{description}

\subsection{Similar solutions}

The audio XAI framework designed and developed as part of the thesis is a unique attempt
to provide an explanation framework that is tailored specifically towards the domain of
audio machine learning. The framework is distinct from other XAI frameworks by providing
direct support for audio features.

To the best of our knowledge, two similar XAI frameworks focused on a similar topic as the thesis,
but as later explained, they do not provide direct support for audio, making them distinct from
the thesis project:
\begin{itemize}
    \item \textbf{explAIner} - The explAIner \cite{Spinner2019-jg} is a visual analytics framework
        for interactive interaction with Explainable AI. It focuses on the whole XAI domain, providing
        support for a wide spectrum of XAI techniques supporting different data types, including text and
        image data. Though it is a very mature tool, it lacks a direct visual support for audio data,
        which distinguishes it from a project for a thesis,
    \item \textbf{XAI - An eXplainability toolbox for machine learning} - It is a toolbox \cite{SomeXaiFramework}
        that provides collection of tools for working with Explainable AI. As with the previous example,
        it is distinct in that it lacks the visual support for audio data.
\end{itemize}

% #############################################
%
% Rozdział 3 - Requirements and tools (może Framework requirements?)
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Requirements and tools} \label{ch:reqrTools}

        This chapter defines and formalizes requirements that the project must actualize
    to meet the objective of the thesis, including software and behavioral requirements.
    The chapter starts with a definition of functional and non-functional requirements,
    followed by an overview of software requirements and tools used in the thesis.

\subsection{Functional requirements}

    The functional requirements seek to define how a framework should behave to its user.
    For clarity the requirements are divided based on the software component of the framework which
    in framework's case are library, user web interface, and terminal helper tool.

% Wymagania interfejsu, backend serwera, UI
\subsubsection{Library application programming interface requirements}
    \begin{enumerate}[itemsep=1\baselineskip]

    \item \textbf{Provide user model injection}

        The library API must enable users to create adapters for their classes allowing them to inject operations on
    their data into the framework. Users may implement custom model adapters by extending the provided base
    classes. This enables integration of various audio model architectures with the explanation
    framework.

    \item \textbf{Provide explanation interface for model adapters}

        The library API must offer base classes for implementing a given explanation method.
    Each adapter type must define specific methods required for the corresponding explanation algorithm,
    including LIME, Integrated gradients, or LRP, ensuring a consistent interface for different explanation methods.

    \item \textbf{Explanation method Selection}

        The library API must offer users the ability to choose which explanation methods are displayed in the view.
    Users can select from available explainers including LIME, Integrated gradients, and LRP through library API or through
    command-line arguments in case when using a helper tool. This allows users to focus on specific explanation
    approaches relevant to their use case.

    \item \textbf{Support necessary audio format}

        The library API must support user audio input provided in \texttt{.wav} and \texttt{.mp4} formats.
    Audio should be automatically converted to appropriate tensor formats for model processing.

    \item \textbf{Provide interface for view classes}

        The library API must offer a base interface for implementing views which will fulfill
    the requirement of modularity and allow a system to support multiple view types.

    \item \textbf{Context Management}

        The library API must define a context that serves to store and save the current explanation progress.
    The context class manages working directories, file operations, and explanation state throughout the process.
    Context provides centralized access to resources and maintains consistency across different explanation phases.

    \item \textbf{Interface for prediction class information}

        The library should offer an interface enabling users to inject prediction class information.
    This information is essential for proper explanation interpretation and visualization.
    The system should support label retrieval dynamically depending on whether the user provided
    support for their user-defined model adapters.

    \item \textbf{Support for selected explanation methods}

        The library API must provide explanation methods selected for the thesis, such as LIME, Integrated gradients, and LRP.
    These methods should be implemented as classes, and they also serve as controllers within
    the Model-View-Controller pattern and coordinate between model adapters, explanation algorithms, and view components. They encapsulate the complete explanation workflow from input processing to result visualization.

    \item \textbf{Support for web interface view}

        The library API must define web server view classes that manage file serving and web-based user interface.
    Web server components handle HTTP requests and serve explanation results through a user interface.

    \item \textbf{Ensure persistent result storage}

        The library API must ensure persistent storage of user results on disk to prevent
    data loss. The context system should provide this by saving explanation artifacts to
    the working directory. This will enable users to archive explanation outputs across 
    multiple sessions.
    
    \end{enumerate}

\subsubsection{User web interface requirements}
    \begin{enumerate}[itemsep=1\baselineskip]
    %\vspace{\baselineskip}
    \item \textbf{Separate tab for each explanation result}

            The user interface must aggregate the results of each explanation method in separate tabs to organize
        outcomes and enable switching between them. Each explanation method, which is LIME, Integrated gradients, or LRP,
        should get its dedicated tab. This should allow users to easily navigate different explanation approaches
        to compare results. The sidebar should provide a structured view of all generated explanations in a single
        interface and allow you to switch between them.

    \item \textbf{Display original audio playback}

        The user interface must display the original audio track that serves as input for explanations across all
    explanation methods. The interface must enable audio playback functionality, including play/pause controls and
    waveform visualization. Users can listen to the original audio while examining explanation results for better
    context understanding. The waveform display provides a visual representation of the audio signal structure.

    \item \textbf{Visualize LIME explanation result}

        The user interface must display LIME explanation result as playable audio.
    Users should be able to listen to specific audio segments that contributed most significantly
    to the model's prediction. This enables an intuitive understanding of which temporal parts of
    the audio influenced the model's decision. The width of the explained audio visualization
    should be aligned with the original audio.

    \item \textbf{Provide version information}

        The user interface must display information about the library version so that users may verify which
    version of a library they are currently using. Such information may prove to be useful,
    for example, for troubleshooting.

    \item \textbf{Display notification and error information}

        The user interface must display notifications about errors or other information in a dedicated information
    panel. The notification system provides user feedback for processing status, errors, warnings,
    and completion messages. Users receive clear communication about the system state and any issues that
    require attention. The dedicated panel ensures important messages are prominently displayed without
    disrupting the main workflow.

    \item \textbf{Visualize attribution intensity}

        The user interface must show plot of attribution intensity of user's input audio
    for both Integrated gradients and LIME explanations. The attribution scores given to various audio input segments are displayed in these plots. This enables users to determine which audio signal regions had the biggest impact on the model's prediction.

    \item \textbf{Display of user's input overlayed with attribution}

        The user interface must display attribution overlaid on user's input for both Integrated gradients and LIME explanations.
    Attribution values are visualized as color heatmaps on the user's spectrogram which enables a user to
    understand which frequency components at a given time interval influenced the model's decision.
    This method works very well when the input provided to the model is a spectrogram.

    \item \textbf{Display prediction classes of user's model}

        The user interface must display optional model label descriptions when provided by the user's model adapter.
    The interpretability of explanation visualizations is improved by label information.

    \end{enumerate}

\subsubsection{Terminal application requirements}
    Framework must provide a terminal-application tool for launching supported models on framework's explanation method with input provided by the user. The tool then will implement a typical usage scenario:
    \begin{enumerate}
        \item User input is converted from a waveform input into model's type which is either tensor of input samples or spectrogram type.
        \item The tool defines a necessary model adapter,
    \end{enumerate}

    \begin{enumerate}[itemsep=1\baselineskip]

    \item \textbf{Provide selection of one or more explanation method}

        The application must enable users to select one or more explanation methods offered by the framework which are LIME, Integrated gradients, or LRP.

    \item \textbf{Provide selection of audio model}

            The application must enable users to select from models implemented within the framework that they
        intend to use. 

    \item \textbf{Provide selection of visualization method}

        The application must enable users to select the visualization method implemented in the framework.
    Users can choose between web-based visualization and disabled visualization.

    \item \textbf{Provide selection of working directory}

    The application must enable users to select a folder as the working directory or create a default when not provided by the user.

    \end{enumerate}

\subsection{Non-functional requirements} \label{ch:NonFuncRequirements}

    The implementation of an audio XAI framework must meet several criteria
    to ensure proper behavior for the user and quality criteria.
    The most important criteria are modularity, usability, and persistency.

\begin{description}
    \item \textbf{Modularity}

         The project thesis aims to provide a general and extensible audio explanation framework. The system must be 
    designed with a modular structure, where key components such as explanation methods, model adapters, and view
    renderers are implemented as independent modules. For that, the system must adhere 
    to the pattern principles of Model-View-Controller architecture as discussed
    in Sec. \ref{ch2:ModelViewController}. This modularity allows for the seamless integration of new 
    functionalities, such as adding additional explanation algorithms or
    supporting new audio models, without requiring  modifications to the core of the system.

    \item \textbf{Reliability}

        The system must behave stably and predictably during normal operation. During regular operation, the system has to
    display stable and predictable behavior. The user interface, audio processing pipelines, and explanation methods must
    all gracefully handle runtime errors and invalid inputs. Errors must be properly logged, and informative feedback must
    be sent to the user without crashing. To avoid data loss, intermediate and final outputs need to be saved consistently.

    \item \textbf{Usability}

        The framework library and web user interface must follow intuitive design principles, including consistent
    navigation, clear labeling, and visual clarity of explanation results. The use of separate tabs for different 
    explanation methods ensures that users can focus on one explanation at a time while easily switching between views. 
    Notifications and status indicators must be visible and should inform a user about the behavior of the interface. In 
    addition, the command-line application helper tool must be a fast way of testing the library implementation and environment setup.

\end{description}

\subsection{Tools specification} \label{ch:ch3LabelSpec}
\begin{description}
\item \textbf{PyTorch}

A comprehensive open-source machine learning framework that provides exceptional flexibility for building and training neural networks with dynamic computational graphs. PyTorch offers seamless GPU acceleration capabilities enabling efficient training of complex deep learning models on large datasets. The framework provides extensive libraries of pre-built components and optimizers that accelerate development while maintaining a pythonic interface. PyTorch has become the preferred framework for research due to its intuitive debugging experience and strong community support.

\item \textbf{librosa}

A specialized Python library for music and audio signal processing that provides comprehensive tools for analyzing sound data through various time-frequency transforms. Librosa offers advanced capabilities for rhythm analysis, beat detection, and feature extraction of key acoustic characteristics commonly used in machine learning pipelines. The library integrates smoothly with numerical computing libraries while supporting various audio formats with built-in resampling capabilities. Librosa emphasizes reproducible research with transparent implementations of audio algorithms complemented by excellent documentation.

\item \textbf{torchaudio}

A domain-specific library built on PyTorch that provides GPU-accelerated tools for audio processing tasks in deep learning applications. TorchAudio offers seamless integration with PyTorch's autograd system, enabling gradient-based training with audio feature extractors as differentiable components. The library includes specialized audio augmentation techniques and pre-trained models for common audio tasks with a consistent tensor-based API. TorchAudio ensures compatibility with PyTorch's ecosystem and is actively maintained with regular updates incorporating state-of-the-art techniques.

\item \textbf{pytest}

A sophisticated testing framework for Python that emphasizes simplicity with a minimalist syntax for writing tests and a powerful fixture system for managing dependencies. Pytest's parameterization features enable running tests with multiple inputs while providing detailed assertion introspection to quickly identify issues. The framework's plugin architecture has fostered a rich ecosystem of extensions for specialized testing needs including coverage analysis. Pytest supports both unit and functional testing with automatic test discovery and comprehensive reporting features.

\item \textbf{matplotlib}

A comprehensive data visualization library for Python that has become the standard for creating publication-quality figures and interactive visualizations. Matplotlib offers exceptional flexibility through its object-oriented API, allowing precise control over every aspect of plots for creating complex custom visualizations. The library supports an extensive range of plot types from basic line graphs to specialized visualizations suitable for diverse data representation needs. Matplotlib integrates seamlessly with NumPy and pandas while providing robust support for mathematical expressions through LaTeX rendering.

\item \textbf{captum}

An interpretability library built specifically for PyTorch that implements state-of-the-art algorithms for explaining decisions made by deep learning models. Captum provides unified implementations of gradient-based attribution methods that reveal feature importance across various model architectures at multiple granularities. The library implements advanced visualization techniques for different data modalities, rendering attributions as heatmaps for images and waveforms for audio. Captum's attribution methods support batch processing and GPU acceleration with a consistent API that works across different model architectures.

\item \textbf{React}

A declarative JavaScript library that has revolutionized user interface development through its component-based architecture and efficient virtual DOM rendering system. React promotes unidirectional data flow that makes application state changes predictable while its JSX syntax combines JavaScript with HTML-like templates for creating reusable components. The library's robust ecosystem includes state management solutions, routing libraries, and testing frameworks that address common development challenges. React's composition-focused design encourages creation of small, focused components that can be combined to build complex interfaces while promoting code reuse.
\end{description}

% #############################################
%
% Rozdział 4 - External specification
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{External specification} \label{ch:externalSpec}

The following chapter provides the external interface of the audio XAI framework implemented as
part of the thesis. It discusses the project's software requirements, installation procedure,
user interface, and framework usage.

\subsection{Software requirements} \label{ch4:DevelopmentSetup}

The audio XAI framework is implemented mainly in the Python programming language and
requires a set of tools from Python's ecosystem for correct setup.
The project's software prerequisites are:

\begin{itemize}
    \item \textbf{Python} - version 3.9.20
    \item \textbf{Conda} - version 24.11.0
    \item \textbf{curl} - version 8.11.0
    \item \textbf{GNU bash} -  version 5.2.37
\end{itemize}

\subsection{Installation procedure}

The framework provides an installation script \texttt{setup.sh} % TODO (Appendix. 3)
in project's root directory that performs full installation of all
software dependencies. To install the project the users must:

\begin{itemize}
    \item Clone the git repository of the audio XAI framework:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{text}
git clone https://github.com/m-falkowski/pylibxai
\end{minted}
    \item Enter the cloned directory:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{text}
cd pylibxai/
\end{minted}
    \item Initialize dependent repositories stored as git's submodules in the repository:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{text}
git submodule update --init --recursive
\end{minted}
    \item Launch the install script that is present in the root directory:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{text}
chmod +x setup.sh && ./setup.sh
\end{minted}
    \item The \texttt{setup.sh} script provides a \emph{conda} environment with all installed dependencies and activates it. The environment can be activated after the installation manually at any given point:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{text}
conda activate pylibxai_context
\end{minted}
\end{itemize}

\subsection{Command-line interface} \label{ch4:CmdTool}

The library provides a 

\subsection{Web interface for explanation}
\subsubsection{Initializing web interface}
\subsubsection{Displaying LIME results}
\subsubsection{Displaying LRP results}
\subsubsection{Displaying Integrated gradients results}
\subsubsection{Displaying software warning} \label{ch4:NotificationPanel}
\subsubsection{Version information}

% #############################################
%
% Rozdział 5 - Framework implementation
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Framework implementation} \label{ch:implementation}

    The following chapter contains an overview of the architecture and implementation details of the explanation framework. It briefly discusses each of the system's components by providing
    a walk-through of each step of the framework's explanation pipeline.

\subsection{Software architecture overview}

    The architecture of the project consists of three main components that are library, web interface, and command-line helper script.
    These component implement all steps of a full-pipeline from preprocessing
    user's input up to visualizing explanation results as discussed in Sec. \ref{ch2:ArchitectureOverview}.

\begin{enumerate}
    \item \textbf{Library} - Provides the main logic and implementation. It is interfacing directly with a user and
    provides the necessary tools for the execution of an explanation. This includes interfaces for the creation of 
    adapters for users' models, logic implementing supported explanation types, visualization of explanations that 
    include a frontend web interface, and helper utilities.

    \item \textbf{Web interface} - Defines a main visual view for the observation of the explanations' results. It is
    composed of two parts which the first is a backend server that serves the explanation results to the frontend part
    through the HTTP protocol. The frontend part then requests the required data from the backend server and visualizes
    it in the form of a browser page. The communication process between user application and framework is shown in
    \ref{fig:CommunicationArchitecture}.

    \item \textbf{Command-line explanation runner} - It is a toolchain's helper script and serves the purpose of quickly 
    launching a typical usage type of the library, which is loading an input audio file and then launching a chosen set 
    of explanations on the chosen library's provided model.
\end{enumerate}

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{comm.jpg}
    \caption{Communication scheme between user application and frontend.}
    \label{fig:CommunicationArchitecture}
\end{figure}

\subsection{Project structure}

    The project’s source code is grouped in multiple directories grouping similar implementations. They contain an implementation of each of the framework’s pipeline phases
    and unit test definitions for them.

\begin{itemize}[itemsep=1\baselineskip]
	\item \textbf{audioLIME/} - Contains implementation of audioLIME explanation method,
	\item \textbf{AudioLoader/} - Provides utilities for loading and preprocessing audio files,
	\item \textbf{Explainers/} - Contains explainer classes for LIME, Integrated gradients, and LRP methods,
	\item \textbf{pylibxai\_explain.py} - Main command-line entry point for running explanations on audio models,
	\item \textbf{Interfaces/} - Defines abstract base classes and interfaces for model adapters and views,
	\item \textbf{model\_adapters/} - Implements adapter classes for integrating different audio models with the framework,
	\item \textbf{models/} - Contains pretrained model files and checkpoints for supported audio models,
	\item \textbf{pylibxai\_context/} - Implements context management for storing explanation state and working directories,
	\item \textbf{Views/} - Contains implementations of different views, such as web view for serving explanation results through HTTP to the web page, or debug view for quick debugging,
	\item \textbf{pylibxai-ui/} - Frontend React application providing web-based user interface for visualization,
\end{itemize}

\subsection{Model-View-Controller implementation overview}

\begin{figure}%[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{class_diagram.png}
    \caption{Library class diagram.}
    \label{fig:ClassDiagram}
\end{figure}

    The thesis utilizes Model-View-Controller (MVC) architectural pattern (Sec. \ref{ch2:ModelViewController}) to obtain extendable architecture that satisfies one of core requirements of the thesis that is modularity as discussed in Sec. \ref{ch:NonFuncRequirements}. This allows a framework to be easily extendable with a new explanation types, machine learning models, and views separating these three from each other so that the developer might work on one of these exclusively.

    The Model-View-Controller is implemented using
    the following classes visible in the class diagram \ref{fig:ClassDiagram}. The implementation and details of these classes are discussed later in the chapter:
    \begin{description}
        \item[Model] The function of the model in the framework is realized using adapter classes for user
        models, such as \texttt{HarmonicCNN} or \texttt{Cnn14} (Sec. \ref{ch2:ModelSelection}). These classes
        operate on user data passed to them from controller classes, performing data manipulation, which is an
        inference in this case. They need to be implemented by the user to provide their support to the framework and they usually come in a form of an adapter class.
        The interface required for implementation of these classes is discussed     
        in Sec. \ref{ch5:AdapterInterfaces},

        \item[View] The function of view in the framework is realized using classes such as \texttt{WebView}
        or \texttt{DebugView}. These classes receive explanation results specified in the \texttt{Context}
        class as required by the \texttt{ViewInterface} class and process them to display the results
        to the user,

        \item[Controller] The purpose of this part is to pass the user’s input into the model
        classes for data manipulation and to later pass the results of data manipulation into 
        the view. This functionality is achieved by classes implementing explainers' logic, 
        such as \texttt{LimeExplainer} or \texttt{IGradientsExplainer}.
    \end{description}

\subsection{Interfaces for model adapters} \label{ch5:AdapterInterfaces}

    Model adapters require a set of interfaces that specify the required behavior that a user
    must fulfill to make their model work properly within the framework. For that reason, the 
    library provides an interface for required explanation methods to perform a given 
    explanation that specifies behavior
    between a given explainer class and the user’s model adapter. This provides enough 
    granularity to the interface so that a user may freely decide which explanation method 
    they would like to provide support for.
    Interfaces for model adapters are implemented with the use of
    Python’s \mintinline{py}{ABC} module \cite{pythonABC}, which allows defining abstract 
    base classes for other classes. Every interface method is annotated 
    \mintinline{py}{@abstractmethod}, requiring a user to define it when performing inheritance, or it throws a \mintinline{py}{TypeError} exception otherwise.
    
\subsubsection{LIME method adapter}

    Adapter for LIME method (\ref{ch2:audioLIME}) defines the interface that the user's model needs to satisfy to be able to launch \texttt{audioLIME} explanation on it.
    This interface requires from user defining a single method \mintinline{py}{get_lime_predict_fn(self)}
    that must return a \mintinline{py}{np.array}.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, xleftmargin=12pt, linenos, mathescape]{py}
from abc import ABC, abstractmethod
import numpy as np

class LimeAdapter(ABC):
    """Abstract base class for LIME (Local Interpretable
       Model-agnostic Explanations) adapters"""
    @abstractmethod
    def get_lime_predict_fn(self) -> Callable[[np.ndarray], np.ndarray]: pass
    """Returns a function that takes an audio input and
       returns the model's prediction for that input."""
\end{minted}
\caption{Class definition of \texttt{LimeExplainer}.}
\label{fig:LimeAdapter}
\end{figure}

\subsubsection{Integrated gradients method adapter}

    In the same way, adapter for Integrated gradients method (\ref{ch2:IgradMethod}) defines the interface that the
    user's model needs to satisfy to be able to launch Integrated gradients method on it.
    This interface requires from user defining a single method \mintinline{py}{get_igrad_predict_fn(self)}
    that must return an \mintinline{py}{np.array}.

    Additionally, Integrated gradients interface requires defining a \mintinline{py}{igrad_prepare_inference_input}
    method that serves a purpose of additionally preprocessing data for a given model as shown in
    \ref{fig:IGradientsAdapter}. This is required as interface for explainers accept user's waveform input but they
    do expect an input which is the same as model's input which may be either waveform or a spectrogram which
    requires additional conversion.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class IGradientsAdapter(ABC):
    """Abstract base class for Integrated gradients method adapters"""
    @abstractmethod
    def get_igrad_predict_fn(self) -> Callable[[torch.Tensor], torch.Tensor]: pass
    """Returns a function that takes an audio input and returns the model's prediction for that input."""

    @abstractmethod
    def igrad_prepare_inference_input(self, x: torch.Tensor) -> torch.Tensor: pass
    """Returns a function that takes an audio input and returns the model's prediction for that input."""
\end{minted}
\caption{Class definition of \texttt{IGradientsAdapter}.}
\label{fig:IGradientsAdapter}
\end{figure}

\subsubsection{LRP method adapter}

    The last method adapter for LRP method (\ref{ch2:LrpMethod}) also requires the user to define a single method \mintinline{py}{get_lime_predict_fn(self)} that must return \mintinline{py}{nn.Module}. This return type is required as Captum's LRP implementation takes it as an input.

    \begin{example}
        An exemplary implementation of LrpAdapter for an exemplary user's adapter \texttt{MyModelAdapter}
        is shown in \ref{fig:UserAdapterExample}.
    \end{example}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
from pylibxai.Interfaces import LrpAdapter

class MyModelAdapter(LrpAdapter):
    def __init__(self, model, device):
        self.model = model
        self.device = device

    def get_lrp_predict_fn(self):
        class MyModelWrapper(torch.nn.Module):
            def __init__(self, model):
                super(MyModelWrapper, self).__init__()
                self.predictor = predictor
                self.device = device

            def forward(self, x):
                self.predictor.model.eval()
                return self.predictor.model(x)

        return MyModelWrapper(self.predictor, self.device)
\end{minted}
\caption{A sample implementation of \texttt{LrpAdapter} for the exemplary \texttt{MyModelAdapter} class.}
\label{fig:UserAdapterExample}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, xleftmargin=12pt, linenos, mathescape]{py}
class LrpAdapter(ABC):
    """Abstract base class for LRP (Layer-wise Relevance Propagation)
       adapters"""
    @abstractmethod
    def get_lrp_predict_fn(self) -> nn.Module: pass
    """Returns a function that takes an audio input and
       returns the model's prediction for that input."""
\end{minted}
\caption{Class definition of \texttt{LrpAdapter}.}
\label{fig:LrpAdapter}
\end{figure}

\subsubsection{ModelLabelProvider interface}

The framework offers an additional interface for model adapters to pass their labels into the system.
The function requires the model to return a dictionary that maps a given class
name to its corresponding identifier, as shown in Fig. \ref{fig:ModelLabelProvider}.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, xleftmargin=12pt, linenos, mathescape]{py}
class ModelLabelProvider(ABC):
    """Abstract base class for providing labels for model predictions"""
    @abstractmethod
    def get_label_mapping(self) -> Dict[int, str]: pass
    """Returns a mapping from label IDs to label names."""

    @abstractmethod
    def map_target_to_id(self, target: str) -> int: pass
    """Maps a target label to its corresponding ID."""
\end{minted}
\caption{Class definition of \texttt{ModelLabelProvider}.}
\label{fig:ModelLabelProvider}
\end{figure}

\subsection{Model adapters} \label{ch5:ModelAdapters}

The framework requires users to provide their ML (machine learning) models
in an adapter class and includes a set of interfaces that define how these
adapters should be implemented (Sec. \ref{ch5:AdapterInterfaces}). These user-supplied classes adjust
model definition and wrap around logic responsible for model inference to allow
them to be used within the framework.

Adapters of users’ ML models serve as model classes in a Model-View-Controller,
where they are responsible for data processing. They are used by controller classes,
which are explainer classes in the case of the framework that pass the user’s input to them.
This way, the data manipulation is separated from the rest of the framework,
and new models may be integrated without modifying the rest of the codebase, which provides modularity.
The user may select individual interfaces that the framework’s API provides,
which allows them to implement only a subset of the functionality that a framework
offers and to skip unneeded parts. The user then may freely decide which interface
they would like to implement for their model between 
\texttt{LrpAdapter}, \texttt{LimeAdapter}, \texttt{IGradientsAdapter}, and \texttt{ModelLabelProvider}.

There are three adapters integrated into the library, which are HarmonicCNN, Cnn14, and GtzanCNN
(Sec. \ref{ch2:ModelSelection}). As their implementation is similar and long, HarmonicCNN is the model
that will described with a precision in the rest of this chapter.

%\subsubsection{HarmonicCNN adapter}
The given adapter first inherits the required interfaces to overload the required methods.
This is shown in Fig. \ref{fig:HarmonicCNNInit}, where the HarmonicCNN class inherits three classes:
\texttt{LimeAdapter}, \texttt{IGradientsAdapter}, and \texttt{ModelLabelProvider}:

The class initialization (Fig. \ref{fig:HarmonicCNNInit}) involves importing the model’s class definition
from the \textit{SotaMusicModels} repository \cite{Won2020-ej}. As the repository hosts a collection
of models and offers the retrieval of a given model through filling \mintinline{py}{config = Namespace()} with
configuration parameters, such as the model name specified as a \mintinline{py}{"hcnn"} string.
The HCNN (HermeticCNN) model is then retrieved \mintinline{py}{self.model = Predict.get_model(config)}.
In the last step, the model state is then loaded using \mintinline{py}{torch.load()} method.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class HarmonicCNN(LimeAdapter, IGradientsAdapter, ModelLabelProvider):
    def __init__(self, device='cuda'):
        # ...
        path_models = os.path.join(path_sota, 'models')

        # ...

        config = Namespace()
        config.dataset = "jamendo"  # we use the model trained on MSD
        config.model_type = "hcnn"
        config.model_load_path = os.path.join(path_models, config.dataset, config.model_type, 'best_model.pth')
        config.input_length = 5 * 16000
        config.batch_size = 1  # we analyze one chunk of the audio
        self.model = Predict.get_model(config)
        
        self.model_state = torch.load(config.model_load_path, map_location=self.device)
        self.model.cuda()
        self.config = config
    # ...
\end{minted}
\caption{Model initialization in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicCNNInit}
\end{figure}

To implement \texttt{ModelLabelProvider} interface, the class creates a mapping of model's class
labels stored in global variable \texttt{TAGS} and later uses them to overload
a method \mintinline{py}{get_label_mapping()} as shown in Fig. \ref{fig:HarmonicLabelMapping}.

\begin{figure}[h!]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
TAGS = ['genre---downtempo', 'genre---ambient', ...]

class HarmonicCNN(LimeAdapter, IGradientsAdapter, ModelLabelProvider):
    def __init__(self, device='cuda'):
        # ...
        self.label_to_id = {i: v for i, v in enumerate(TAGS)}
        self.id_to_label = {v: i for i, v in enumerate(TAGS)}
        # ...
    def get_label_mapping(self):
        """Returns the label mapping for the model."""
        return self.label_to_id
    # ...
\end{minted}
\caption{Set up of label classes in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicLabelMapping}
\end{figure}

The adapter defines a method \mintinline{py}{get_igrad_predict_fn()} that implements
the \texttt{IGradientsAdapter} interface as shown in Fig. \ref{fig:HarmonicIgradOverload}.
This method must return a function with a type \mintinline{py}{Callable[[torch.Tensor], torch.Tensor]}, % TODO: overflow hbox
that takes Pytorch \mintinline{py}{torch.Tensor} both as an argument and return type.
The callback then defines an inner function \mintinline{py}{predict_fn(x)} that contains
the standard inference code of HarmonicCNN, and then it returns that function.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def get_igrad_predict_fn(self) -> Callable[[torch.Tensor], torch.Tensor]:
    # ...
    def predict_fn(x):
        # ... 
        output_dict = self.model(x)
        output_tensor = output_dict
        return output_tensor
    return predict_fn
\end{minted}
\caption{Supporting Integrated gradients method in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicIgradOverload}
\end{figure}

To implement \texttt{LrpAdapter} the adapter provides a definition for LRP
callback as shown in Fig. \ref{fig:HarmonicLrpOverload}. This method must return
a function object that returns a Python's \mintinline{py}{torch.nn.Module} meaning
that a user must create a wrapper class around it's model's implementation.
The function defines a \mintinline{py}{HarmonicCNNWrapper} that inherits from 
\mintinline{py}{torch.nn.Module} class and provides a required
\mintinline{py}{forward()} method where a HarmonicCNN inference is placed.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def get_lrp_predict_fn(self):
    class HarmonicCNNWrapper(torch.nn.Module):
        def __init__(self, model, device, model_state):
            super(SotaNNWrapper, self).__init__()
            self.model_state = model_state
            self.model = model 
            self.device = device
        def forward(self, x):
            #...

            output_dict = self.model(x)
            output_tensor = output_dict
            return output_tensor
    return HarmonicCNNWrapper(self.model, self.device, self.model_state)
\end{minted}
\caption{Support of LRP method in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicLrpOverload}
\end{figure}

The last implementation is a support for the LIME method in Fig. \ref{fig:HarmonicLLIMEOverload}.
This method must return a function with a type \mintinline{py}{Callable[[np.ndarray], np.ndarray]}
to fulfill the implementation of framework’s \texttt{LimeExplainer} which is based on \textit{audioLIME}
implementation mthat operates on numpy’s array types. The function then must
take a numpy \texttt{np.ndarray} as an input and also return it.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def get_lime_predict_fn(self) -> Callable[[np.ndarray], np.ndarray]:
    # ...

    def predict_fn(x_array):
        # convert numpy array into tensor
        audio = torch.zeros(len(x_array), self.config.input_length)
        for i in range(len(x_array)):
            audio[i] = torch.Tensor(x_array[i]).unsqueeze(0)
        audio = audio.cuda()
        audio = Variable(audio)
        output_dict = self.model(audio) # inference here (input is passed into model)
        output_tensor = output_dict.detach().cpu().numpy()
        return np.array(output_tensor)

    return predict_fn
\end{minted}
\caption{Support of LIME method in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicLLIMEOverload}
\end{figure}

%\subsubsection{CNN14 adapter} TODO: potrzebne?
%\subsubsection{GtzanCNN adapter} TODO: potrzebne?

\subsection{Explainer classes}

The explainer classes are the core component of the framework that is responsible
for the launching of the explanation methods on the user input and the
selected model. Their primary purpose is to manage perform explanation
on the user's model and manipulation of the resulting data based on the users decision.
They offload from the user a need to visualize 

These classes serve a purpose of controllers as part of the Model-View-Controller
pattern though this name is not taken directly in their classes names.
As controllers they manage the model and view classes and they

Explanation methods are implemented as separate classes including \texttt{LimeExplainer},
\texttt{IGradientsExplainer}, and \texttt{LrpAdapter}.

\subsubsection{Integrated gradients and LRP explainers}

The explainer implementation for integrated gradients and LRP are both attribution-based methods
%as they both use implementation from the Captum library which
where they both return the attribution map as a result of their explanation as discussed in Sec. \ref{ch2:IgradMethod}.

Both implementation define the ...

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class IGradientsExplainer:
    def __init__(self, model_adapter, context, device, view_type=None, port=9000):
        # ...
        self.model_adapter = model_adapter
        predict_fn = model_adapter.get_igrad_predict_fn()
        self.explainer = IntegratedGradients(predict_fn)
        # ...
        self.view_type = view_type
        if view_type == ViewType.WEBVIEW:
            self.view = WebView(context, port=port)
        elif view_type == ViewType.DEBUG:
            self.view = DebugView(context)
        elif view_type == ViewType.NONE:
            self.view = None
        else:
            raise ValueError(f"Invalid view type: {view_type}. Must be one of WEBVIEW, DEBUG, or NONE.")
\end{minted}
\caption{Class definition of \texttt{IGradientsExplainer}.}
\label{fig:IGradientsExplainer}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class IGradientsExplainer:
    def explain_instance(self, audio, target, background=None):
        audio = self.model_adapter.igrad_prepare_inference_input(audio)
        attributions, delta = self.explainer.attribute(audio, target=target, return_convergence_delta=True)
        return attributions, delta
\end{minted}
\caption{Class definition of \texttt{IGradientsExplainer}.}
\label{fig:explain_instance}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class IGradientsExplainer:
    def explain_instance_visualize(self, audio, target, type=None, background=None, attr_sign='positive'):
        audio = self.model_adapter.igrad_prepare_inference_input(audio)
        attributions, delta = self.explainer.attribute(audio, target=target, return_convergence_delta=True)
        # ...
        plt.ioff()
        return viz.visualize_image_attr(attributions,
                                        audio,
                                        type,
                                        attr_sign,
                                        fig_size=(24,16),
                                        show_colorbar=True,
                                        outlier_perc=50)
\end{minted}
\caption{Class definition of \texttt{IGradientsExplainer}.}
\label{fig:explain_instance_visualize}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class IGradientsExplainer:
    def get_smoothed_attribution(self):
        def moving_average(data, window_size=15):
            return np.convolve(data, np.ones(window_size)/window_size, mode='same')

        attribution = self.attribution.squeeze()
        positive_attribution = torch.clamp(attribution, min=0.0)
        summed_attribution = positive_attribution.sum(dim=0).detach().cpu().numpy()  # Shape: [1292]
        smoothed_attribution = moving_average(summed_attribution)
        return smoothed_attribution
\end{minted}
\caption{Class definition of \texttt{IGradientsExplainer}.}
\label{fig:get_smoothed_attribution}
\end{figure}

\subsubsection{LIME explainer}

% ...

\subsection{Context class definition}

The \texttt{PylibxaiContext} class ma`nages the data and encapsulates the data saving process.
It is an important bridge between model and view classes, formalizing how explanation results
are saved and accessed through a framework.
The class also implements the requirement \textit{Context Management}
and \textit{Ensure persistent result storage} as described
in Sec. (\ref{ch:NonFuncRequirements}), fully encapsulating the process of
saving the explanation results into the persistent storage. For the simplicity of design and also the purpose of serving
explanation result to the web frontend as discussed later in \ref{ch:ch5WebFrontend},
the implementation of this class saves all explanation results as files in a tree structure in a given working directory.

\begin{example}
    An exemplary implementation of the \texttt{write\_attribution} method that is defined in the
    \texttt{PylibxaiContext} class. The method saves an attribution of the explanation to a file in \texttt{JSON} format
    as shown in Fig. \ref{fig:WriteAttributionMethod}.
\end{example}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def write_attribution(self, smoothed_attribution, suffix):
    path = os.path.join(self.workdir, suffix)
    with open(path, 'w') as f:
        json.dump({
            "attributions": smoothed_attribution.tolist(),
        }, f, indent=4)
\end{minted}
\caption{The implementation of \texttt{write\_attribution} method of \texttt{PylibxaiContext} class.}
\label{fig:WriteAttributionMethod}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class PylibxaiContext:
    def __init__(self, workdir):
        self.workdir = workdir
        # ...
    
    def write_plt_image(self, fig, suffix): pass # ...
    
    def write_attribution(self, smoothed_attribution, suffix): pass # ...

    def write_label_mapping(self, labels, suffix): pass # ...
    
    def write_audio(self, audio, suffix, *args, **kwargs): pass # ...
\end{minted}
\caption{Class definition of \texttt{PylibxaiContext}.}
\label{fig:PylibxaiContext}
\end{figure}

The \texttt{PylibxaiContext} class contains the following fields, as shown in its definition in Fig. \ref{fig:PylibxaiContext}:

\begin{itemize}
    \item \mintinline{py}{self.workdir} - the field holds a path to the working directory in which explanation results will be saved,
    \item \mintinline{py}{write_plt_image(self, fig, suffix)} - it handles saving the results of Matplotlib's (Sec. \ref{ch:ch3LabelSpec}) generated image,
    \item \mintinline{py}{write_attribution(self, smoothed_attribution, suffix)} - it handles saving explanation's attribution,
    \item \mintinline{py}{write_label_mapping(self, labels, suffix)} - it handles saving the mapping of labels into their indexes,
    \item \mintinline{py}{write_audio(self, audio, suffix, *args, **kwargs)} - it handles saving of an audio file,
\end{itemize}

\subsection{View classes}

The purpose of view classes is to present explanation results to the user in a readable form
according to Model-View-Controller pattern. The framework is implemented with modularity as a goal
and allows multiple views defined implementing \texttt{ViewInterface} interface.
The framework is implemented with modularity as a goal and allows multiple views
defined by implementing \texttt{ViewInterface} interface that defines how explanation
classes should be implemented in the framework.
This interface \texttt{ViewInterface} requires a user to define the necessary
methods required to initialize, start, and stop the given view as shown in Fig. \ref{fig:ViewType}.

\begin{itemize}
    \item \mintinline{py}{__init__(self, context)} - A given view is initialized with a provided context class.
    The view then may utilize the accumulated results of explanation in that class and present them to the user,

    \item \mintinline{py}{start(self)} - The start method is responsible for launching the given view. In the case of \texttt{WebView},
    it starts the backend and frontend servers. For \texttt{DebugView} it generates a terminal debugging logs showing based on 
    explanation results,
    
    \item \mintinline{py}{stop(self)} - The stop method is responsible for stopping and cleaning the execution of a given view class. It is required though it logic may be empty when there is nothing to stop as in case of \texttt{DebugView} class.
\end{itemize}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class ViewInterface(ABC):
    """Abstract base class for View Interface
    """
    @abstractmethod
    def __init__(self, context):
        self.context = context

    @abstractmethod
    def start(self) -> None: pass

    @abstractmethod
    def stop(self) -> None: pass    
\end{minted}
\caption{Class definition of \texttt{ViewInterface}.}
\label{fig:ViewInterface}
\end{figure}

Every view class defined in a framework must have an entry in enumeration \texttt{ViewType}.
For \texttt{WebView} there is an enumeration \mintinline{py}{DEBUG} provided, for \texttt{DebugView}
it is \mintinline{py}{DEBUG} as shown in Fig. \ref{fig:ViewType}.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class ViewType(IntEnum):
    """Enum-like class for View Types"""
    WEBVIEW = 1
    DEBUG = 2
    NONE = 2
\end{minted}
\caption{Class definition of \texttt{ViewType}.}
\label{fig:ViewType}
\end{figure}

\subsubsection{Web view}

The \texttt{WebView} implementation provides the main visualization of the explanation results
in the form of a web page. Its implementation only starts the underlying backend and frontend servers
in separate processes using Python's subprocess module.  The class implements the View interface to be later used within a framework. The definition of \texttt{WebView} is visible in Fig. \ref{fig:WebView}.

The whole startup process of the WebView contained in the \mintinline{py}{setup()} method contains the following steps:
\begin{enumerate}
    \item The startup procedure starts with the creation of a backend server using the function  \\
        \mintinline{py}{run_file_server(self.context.workdir, self.port)}.
        It takes the path of \\ the working directory from a given instance of the \texttt{Context} class
        and start a file serving server using Python's \mintinline{py}{socketserver.TCPServer} class.
        The implementation of file server is discussed later in Sec. \ref{ch5:FileServer},
    \item In the next step, the frontend server is initialized with the use of the Vite \cite{ViteDOC} tool,
        which it is based on. The \texttt{subprocess.Popen} argument takes the
        Vite command startup arguments as \mintinline{py}{['npm', 'run', 'dev']} that is run in the current working directory where the implementation of the frontend source code is placed, which is \mintinline{py}{cwd=self.vite_dir}. Additional parameters for the frontend web server are passed through the environmental variables, such as a port number that is passed to the frontend through the \mintinline{py}{'VITE_PYLIBXAI_STATIC_PORT'} environmental variable.
    
\end{enumerate}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class WebView(ViewInterface):
    def __init__(self, context, port=9000):
        super().__init__(context, port)
        self.vite_dir = get_install_path() / "pylibxai" / "pylibxai-ui"
        self.server = None
        self.vite_process = None

    def start(self):
        # Start the file server
        self.server = run_file_server(self.context.workdir, self.port)
        # Start the Vite UI
        env = os.environ.copy()
        env['VITE_PYLIBXAI_STATIC_PORT'] = str(self.port)
        print(f"Vite UI directory: {self.vite_dir}")
        self.vite_process = subprocess.Popen(
            ['npm', 'run', 'dev'],
            cwd=self.vite_dir,
            env=env,
            stdout=sys.stdout,
            stderr=sys.stderr,
            shell=True
        )
        print(f"Vite UI launched at http://localhost:{self.port}/ (UI dev server running)")

    def stop(self):
        if self.vite_process:
            self.vite_process.terminate()
            self.vite_process.wait()
            print("Vite UI process terminated.")
        if self.server:
            self.server.shutdown()
            print("File server stopped.")
\end{minted}
\caption{Class definition of \texttt{WebView}.}
\label{fig:WebView}
\end{figure}
 
\subsubsection{Debug View}

The purpose of the\texttt{DebugView} class is to enable quick debugging of the explanation
framework, displaying information about results to a terminal without launching any external windows or software.
It also shows an alternative view implementation to the \texttt{WebView} that shows the modularity of the explanation framework.

The implementation iterates over the content of the context class and displays its content in a structured manner to the user
using Python's \mintinline{py}{print()} function, as shown in the class implementation in Fig. \ref{fig:DebugView}. It does not contain any cleanup functionality, hence its \mintinline{py}{stop()} method has an empty body.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class DebugView(ViewInterface):
    def __init__(self, context):
        super().__init__(context)
        self.context = context

    def start(self):
        print("=== DEBUG VIEW: PylibxaiContext Content ===")
        print(f"Working directory: {self.context.workdir}")
        print()
        
        # Display directory structure
        print("Directory structure:")
        self._print_directory_tree(self.context.workdir)
        print()
        
        # Display content of each subdirectory
        subdirs = ["integrated-gradients", "lrp", "lime"]
        for subdir in subdirs:
            subdir_path = os.path.join(self.context.workdir, subdir)
            if os.path.exists(subdir_path):
                print(f"=== {subdir.upper()} Directory Content ===")
                self._display_directory_content(subdir_path)
                print()
        
        # Display any JSON files in the root directory
        print("=== Root Directory Files ===")
        self._display_directory_content(self.context.workdir, root_only=True)
        print()
        
        print("=== DEBUG VIEW: Content Display Complete ===")

    def stop(self):
        pass
\end{minted}
\caption{Class definition of \texttt{DebugView}.}
\label{fig:DebugView}
\end{figure}

\subsection{Backend Server implementation} \label{ch5:FileServer}

The backend server implementation is implemented using Python's built-in \texttt{TCPServer} class from
\texttt{socketserver} module. It allows serving files placed in a given directory provided as an input.
This allows serving explanation results that are contained in an instance of \texttt{PylibxaiContext} that
are passed to the view class from the framework.

Initially, the system path is changed into the working directory as shown in Fig. \ref{fig:BackendServer},
so that the server is launched serving the files inside, creating endpoints for each stored resource.
The advantage of such an implementation is automatic scalability with the content of a working directory,
which provides simplicity of the implementation. The server is launched in a separate thread using the 
\mintinline{py}{threading.Thread} module over a given port provided as an input
\mintinline{py}{socketserver.TCPServer(("", port), handler)}.

Additionally, the server sets up a minimal CORS access control rules by defining a custom
class \texttt{CORSHTTPRequestHandler} as shown in Fig \ref{fig:CORSHTTPRequestHandler}.

\begin{example}
    An exemplary explanation result which is explanation's attribution saved as a file \texttt{workdir/lrp/attribution.json}
    will have an HTTP endpoint generated by the backend server of \texttt{/lrp/attribution.json}.
\end{example}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def run_file_server(directory, port=9000):
    """Start a file server in a background thread."""
    os.chdir(directory)

    handler = CORSHTTPRequestHandler
    httpd = socketserver.TCPServer(("", port), handler)

    print(f"Serving files from {directory} at http://localhost:{port}/")

    # Run in background thread
    thread = threading.Thread(target=httpd.serve_forever, daemon=True)
    thread.start()

    return httpd  # To stop later with httpd.shutdown()

\end{minted}
\caption{Implementation of backend server.}
\label{fig:BackendServer}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class CORSHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def end_headers(self):
        # Add CORS headers
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'X-Requested-With')

        
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        return super().end_headers()

    def do_OPTIONS(self):
        # Handle preflight requests
        self.send_response(200)
        self.end_headers()
\end{minted}
\caption{Class definition of \texttt{CORSHTTPRequestHandler}.}
\label{fig:CORSHTTPRequestHandler}
\end{figure}

\subsection{Web frontend implementation} \label{ch:ch5WebFrontend}

The main visualization of explanation results is created in the form of a web page displaying different results of
an explanation, such as audio waveform visualization, explanation’s attribution, or spectrograms graphically in a
browser. The frontend implementation is built using the React JavaScript library \cite{ReactDOC}
and Bootstrap CSS framework \cite{BootstrapDOC} and follows a component-based architecture where each interface
element is encapsulated as a reusable React component

%\subsubsection{Frontend layout organization}

The main objective of web page design is to offer a separate display of each explanation method to not
confusing the results with each other, as discussed in Sec. \ref{ch:ch3LabelSpec}.
The Web page’s layout is organized to emphasize the separation of different explanation methods to allow
a user to quickly switch between them. For that, the web page organizes different data displayed in
separate sections and provides a sidebar to switch between them.

The organization of the website's layout is shown in Fig. \ref{fig:PageLayout} and includes:

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{page_layout.png}
    \caption{The organization of website. Each components is identified by a number.}
    \label{fig:PageLayout}
\end{figure}

\begin{itemize}
    \item \textbf{Sidebar (1)} - Sidebar allows users to navigate between different sections, including mode
        information, Integrated gradients, LIME, and LRP explanations, using a list of buttons, highlighting
        the actively shown tab,
    \item \textbf{Navigation bar (2)} - The Navigation bar is a permanently visible page element that is sticky
        to its upper side. It contains various control icons on its left side, including a notification
        bell icon or a help icon,
    \item \textbf{Notification panel and help (3)} - these utilities elements are provided
        to a user to show them helper information such as framework's versioning or any errors
        and notifications that were encountered through the process of explanation visualization.
        The exemplary content of notification panel is discussed in the Sec.\ref{ch4:NotificationPanel}. 
    \item \textbf{Main content area (4)} - Main content area display currently selected section
        where the data displayed is organized vertically in a folded sections, allowing a user
        to hide unwanted information. Displayed information include model information, audio playback,
        attribution visualization, or explanation results.
\end{itemize}

\subsubsection{Frontend component overview}

The implementation consists of multiple components,
each defined in a separate file, including:

\begin{itemize}[itemsep=1\baselineskip]
	\item \textbf{App} - Main application component that manages global state,
        including managing the notification queue or the currently selected page to render,
    \item \textbf{Navbar} - Fixed navigation bar displaying supporting components,
        including notification bell, or version details,
    \item \textbf{ContentPage} - Manages rendering of the main content area and
        routing between different result sections of the interface,
    \item \textbf{ModelInfo} - Component that displays model information,
        such a label mappings, presenting their names along with a class identifier,
    \item \textbf{Lime, Igradients, Lrp} - Each of these components manages the display of its
        respective explanation method’s result. This includes displaying audio playback,
        attribution visualizations, or interactive charts,
    \item \textbf{Footer} - Provides footer of the web page.
\end{itemize}

Frontend retrieves explanation results data from the backend server that was set up by the
\texttt{WebView} class to serve collected data through the process of explanation.
Both servers are set up by the view class and operate on localhost. 
Frontend retrieves the backend server's port from environmental variables using the \mintinline{js}{meta.env}
module:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
import.meta.env.VITE_PYLIBXAI_STATIC_PORT
\end{minted}

Based on that, we define a URL of the backend server from which all frontends’ requests
will be made, which is stored in a variable:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
staticBaseUrl = `http://localhost:\${staticPort}`.
\end{minted}
The requests are done using JavaScript's built-in Fetch API utilizing \mintinline{js}{fetch()} method.
An example of data fetching from backend which is fetching of explanation's attribution
contained in a file in a JSON format as seen in Fig. \ref{fig:IgradComponentDataFetching}.

An instance of an HTTP request is generated using the \mintinline{js}{fetch()} method. After that, the data is processed further by the frontend to create a visualization of it
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
const response = await fetch(`\${staticBaseUrl}/igrad/igrad_attributions.json`)
\end{minted}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{js}
function Igradients() {
  // [...]

  // Get the static file server port from environment variables
  const staticPort = import.meta.env.VITE_PYLIBXAI_STATIC_PORT || '9000'
  const staticBaseUrl = `http://localhost:${staticPort}`

  // [...]

  useEffect(() => {
    // Fetch Integrated gradients attributions data
    const fetchAttributions = async () => {
      try {
        setIsLoading(true)
        const response = await fetch(`${staticBaseUrl}/igrad/igrad_attributions.json`)
        if (!response.ok) {
          throw new Error(`HTTP error! Status: ${response.status}`)
        }
        const data = await response.json()
        if (!data.attributions || !Array.isArray(data.attributions)) {
          throw new Error('Data is not in the expected format (object with attributions array)')
        }
        setAttributions(data.attributions)
        setIsLoading(false)
      } catch (error) {
        setIsLoading(false)
        pushNotification({ type: 'error', message: `Integrated gradients: ${error.message}` })
      }
    }
    fetchAttributions()
  }, [staticBaseUrl, pushNotification])
}
\end{minted}
\caption{Data fetching logic defined in a \texttt{Igradients} component.}
\label{fig:IgradComponentDataFetching}
\end{figure}
    
\subsubsection{Visualization of audio waveform}

The frontend provides visualization and audio playback functionality of
different audio pieces that appear through the process of explanation:

\begin{itemize}
    \item Original input - it is visualized so that a user access and listen
          to the original input,
    \item Lime method explanation - the LIME method provides a result in a form of an audio
          where an explanation which is the part of input that was the most important for
          the decision of explanation is preserved in the audio and the rest is muted.
\end{itemize}

The visualization and playback of the audio pieces is implemented using the Wavesurfer.js
library \cite{WavesurferDOC}. It is used in the \texttt{Lime}, \texttt{Igradients}, and \texttt{Lrp}
components to visualize the input audio, and the LIME method component also uses it for the
visualization of its explanation. The advantage of the Wavesurfer.js library is that it
provides interactive playback functionality meaning that a user may listen to the original
audio and select any moment of the audio to listen to.

The exemplary initialization of WaveSurfer's audio playback in the \texttt{Igradients} component is shown
in Fig. \ref{fig:IgradientsAudioPlayback}. The use is analogous for the \texttt{Lime} and \texttt{Lrp} components.
An audio playback defined in this way will generate the following view as shown Fig. \ref{fig:AudioPlayback}.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{js}
function Igradients() {
  // [...]

  wavesurfer.current = WaveSurfer.create({
    container: waveformRef.current,
    waveColor: '#4F4A85',
    progressColor: '#383351',
    cursorColor: '#383351',
    barWidth: 2,
    barRadius: 3,
    responsive: true,
    height: 100,
    barGap: 3
  })

  // [...]
}
\end{minted}
\caption{Audio playback definition in a \texttt{Igradients} component.}
\label{fig:IgradientsAudioPlayback}
\end{figure}

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{audio_playback.png}
    \caption{The visualization of audio playback.}
    \label{fig:AudioPlayback}
\end{figure}

%\subsubsection{Visualization of attribution intensity}

The frontend must also provide a visualization of intensity of the attribution map
(Sec. \ref{ch2:IgradMethod}) over time.
This functionality is implemented using ChartJS library \cite{ChartJsDoc}.
The attribution intensity is a summation of all waveform frequencies over a time so it may
be displayed using a simple linear plot that the ChartJS library provides.
The exemplary attribution plot definition is shown in Fig. \ref{fig:IgradientsAttrPlot}
and the generated plot corresponding to it is shown in Fig. \ref{fig:AttrPlot}.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{attribution_plot.png}
    \caption{The linear plot displaying attribution during time of an audio.}
    \label{fig:AttrPlot}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{js}
    chartInstance.current = new Chart(ctx, {
      type: 'line',
      data: {
        labels: labels,
        datasets: [{ label: `Integrated gradients Attribution Values`, data: attributions,
          fill: false, borderColor: 'rgb(42, 123, 198)', tension: 0.1
        }]
      },
      options: {
        responsive: true,
        maintainAspectRatio: false,
        plugins: {
          title: { display: true, text: 'Integrated gradients Attributions' },
          tooltip: {
            callbacks: {
              title: (tooltipItems) => { return `Frame: ${tooltipItems[0].label}` },
              label: (tooltipItem) => { return `Value: ${tooltipItem.raw.toFixed(4)}` }
            }
          }
        },
        scales: {
          x: { title: { display: true, text: 'Frame Index' } },
          y: { beginAtZero: true, title: { display: true, text: 'Attribution Value' }
          }
        }
      }
    })
\end{minted}
\caption{Definition of attribution plot defined in the \texttt{Igradients} component.}
\label{fig:IgradientsAttrPlot}
\end{figure}

\subsubsection{Error and Notification system}

The notification panel is responsible for displaying to the user any notifications
and errors that appear during the execution of the frontend. It is implemented as
a fixed-size bounded queue of size 10, meaning than any new errors that would exceed
the maximal limit of the queue.

The logic of pushing any new notification into the queue is implemented in the function
\mintinline{js}{pushNotification()} as shown in Fig. \ref{fig:NotificationImpl}.
The function adds new notification at the start of the array:

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
const next = [
  { id: Date.now() + Math.random(), ...notification },
    ...prev
  ];
\end{minted}

It then discards the oldest notifications when the limit exceeds the maximum size:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
next.slice(0, maxNotifications);
\end{minted}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{js}
// Notification context
export const NotificationContext = createContext({
  notifications: [],
  pushNotification: () => {},
});

function App() {
  const [notifications, setNotifications] = useState([]); // {id, type, message}
  const maxNotifications = 10;
  const handleNavigation = (section) => {
    setCurrentSection(section);
  };

  // Push notification (circular queue)
  const pushNotification = useCallback((notification) => {
    setNotifications(prev => {
      const next = [
        { id: Date.now() + Math.random(), ...notification },
        ...prev
      ];
      return next.slice(0, maxNotifications);
    });
  }, []);
}
\end{minted}
\caption{The implementation of ring-buffer responsible for storing notifications.}
\label{fig:NotificationImpl}
\end{figure}

Later, the notification queue may be used across the frontend code to handle different
events that have occurred. The user provides a type of event into the \mintinline{js}{'type'} key and
the message into the \mintinline{js}{'message'} key:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
pushNotification({ type: 'error', message: `Integrated Gradients: Failed to load audio: ${error.message}` })
\end{minted}

\clearpage % TODO: usuń
\subsection{Explanation runner command-line script}

The framework provides a command-line tool script as a helper tool for quick execution
of explanation methods on the user’s input using models integrated into the library
(Sec. \ref{ch5:ModelAdapters}). The full script implementation
is shown in Appendix. \ref{appendix:ScriptListing}.

For this, the script sets up terminal options as described in Sec. \ref{ch4:CmdTool}, with a use of Python's \texttt{argparse.ArgumentParser} class:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
parser = argparse.ArgumentParser(description="Process a model name and input path.")
    
parser.add_argument('-m', '--model', type=str, required=True,
                    help="Name of the model to use [{sota_music, paans, gtzan},]...")
# ...
args = parser.parse_args()
\end{minted}

Then, to launch a given explanation method, an instance of the \texttt{PylibxaiContext} class
is set up with a directory provided by the user through command-line options:

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
context = PylibxaiContext(args.workdir)
\end{minted}

The script then initializes the appropriate model adapter provided by the user through the \texttt{-m/--model} parameter: 
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
if args.model == "HCNN":
    adapter = HarmonicCNN(device=device)
elif args.model == "CNN14":
    adapter = Cnn14Adapter(device=device)
elif args.model == "GtzanCNN":
    adapter = GtzanCNNAdapter(model_path=GTZAN_MODEL_PATH, device=device)
else:
    print('Invalid value for -m/--model argument, available: [HCNN, CNN14, GtzanCNN].')
    return
\end{minted}

The script then sets up the web page view if it was selected or defaults to debug view otherwise: 
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
view_type = ViewType.WEBVIEW if args.visualize else ViewType.DEBUG
\end{minted}

Then, the initial files are saved into the current context, which includes original audio input
and a mapping of class labels into their identifiers. The mapping is performed conditionally,
whether the model adapter implements \texttt{ModelLabelProvider}.

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
context.write_audio(args.input, os.path.join("input.wav"))
if issubclass(type(adapter), ModelLabelProvider):
    context.write_label_mapping(adapter.get_label_mapping(), os.path.join("labels.json"))
\end{minted}

Finally, after setting up required classes and parsing all command-line options,
the script invokes selected explanation methods by running their
explainer class on the user’s input:

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
if "lime" in expls:
    view = view_type if expl_count == 1 else ViewType.NONE
    expl_count -= 1
    explainer = LimeExplainer(adapter, context, view_type=view)
    explainer.explain(args.input, target=None)
if "lrp" in expls:
    # ...
if "integrated-gradients" in expls:
    # ...
\end{minted}

% #############################################
%
% Rozdział 6 - Verification
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Verification and validation} \label{ch:verification}

This chapter presents testing performed to ensure the correctness of the Audio XAI framework
implemented as part of the thesis. It also summarizes the results obtained during thesis implementation,
particularly the support of the chosen ML models and the explanation methods.
The project unit tests have been implemented using \emph{pytest} \cite{PytestDoc}, a testing framework for Python.

%\subsection{Case study: integrating Simple GTZAN model into framework} TODO: Robić???
\subsection{Explanation support for models} \label{ch6:ExplanationSupport}

The thesis aimed to provide suitable explanation methods and integrated models to give a base functionality
to work with the framework. The implemented explanation methods by the framework were implemented for
each of the integrated models. The results of the supported explanation framework for the implemented models
in the framework are shown in the Table. \ref{table:AvalibilityTable} where \emph{supported} methods by the model
are marked using the symbol \cmark and \emph{unsupported} methods using \xmark. Some explanation methods had not
been launched properly in some models due to various integration issues that were caused either by the development
setup (Sec. \ref{ch4:DevelopmentSetup}) or library issues.

\begin{table}[h]\centering
    \caption{Availability of Explanation Methods per Model}
    \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{LIME} & \textbf{Integrated Gradients} & \textbf{LRP} \\
    \midrule
    CNN14         & \cmark        & \cmark                         & \xmark       \\
    GtzanCNN      & \xmark        & \cmark                         & \cmark       \\
    HarmonicCNN   & \cmark        & \cmark                         & \xmark       \\
    \bottomrule
    \end{tabular}
\label{table:AvalibilityTable}

\hfill

\textbf{Where:} \cmark\ - method has been implemented successfully; \xmark\ - method has not been implemented.
\end{table}

Firstly, the LIME method has been implemented for the \emph{GtzanCNN} model, but launching any
input on this model required more memory than the GPU on the development setup and
caused a memory error, as shown in Fig. \ref{fig:LimeGtzanCNN}.
%This issue is because \emph{GtzanCNN}, in contrast to other supported models,
%uses a spectrogram as an input rather
%than taking a waveform, which is a 1D consecutive tensor, and converts it internally to the spectrogram.
%Spectrogram tensor is a 2D tensor having a much bigger memory footprint, which causes a LIME
%implementation to allocate significantly more memory.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{text}
$ python pylibxai_explain.py -w ./gtzancnn_expl/ -m GtzanCNN --explainer=lime --target=jazz -i ../data/gtzan_jazz.wav
Colocations handled automatically by placer.
Creating explanation object
Starting LIME explanation
Traceback (most recent call last):
  File "pylibxai\pylibxai\pylibxai_explain.py", line 94, in <module>
    main()
  # Skipped stack trace
  File "torch\nn\functional.py", line 5209, in pad
    return torch._C._nn.pad(input, pad, mode, value)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.05 GiB. GPU 0 has a total capacity of 15.99 GiB of which 0 bytes is free. Of the allocated memory 25.28 GiB is allocated by PyTorch, and 77.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
\end{minted}
\caption{The out of memory error occurring during LIME explanation on \emph{GtzanCNN} model.}
\label{fig:LimeGtzanCNN}
\end{figure}

In the second case, the implementation of the Layer-wise relevance propagation method has been impossible for
\emph{CNN14} and \emph{GtzanCNN} models. This is because the LRP implementation, the one used from the
\emph{Captum} framework, is based on a backward propagation mechanism that is sequentially applied to every
model. \emph{Captum} requires per-layer rules defined for each of the layers used in machine learning models.
Audio models frequently contain audio-specific layers performing, for example, waveform to spectrogram conversion,
as in the case of the layer \texttt{torchlibrosa.stft.LogmelFilterBank} in \emph{CNN14} and \emph{GtzanCNN}. 
This disallows the LRP algorithm from running as shown in Fig. \ref{fig:LrpMissingLayer},
and such layers’ enablement in the Captum library is out of scope of the thesis.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{text}
  File "pylibxai\captum\captum\attr\_core\lrp.py", line 202, in attribute
    self._check_and_attach_rules()
  File "pylibxai\captum\captum\attr\_core\lrp.py", line 313, in _check_and_attach_rules
    raise TypeError(
TypeError: Module of type <class 'torchlibrosa.stft.LogmelFilterBank'> has no rule defined and nodefault rule exists for this module type. Please, set a ruleexplicitly for this module and assure that it is appropriatefor this type of layer.
\end{minted}
\caption{Missing rule error for layer \texttt{torchlibrosa.stft.LogmelFilterBank} of \emph{CNN14} model.}
\label{fig:LrpMissingLayer}
\end{figure}

\subsection{Scope of testing}

The scope of testing includes unit tests of the given framework's components and integration tests.
The test setup for framework's tests is the same as for the development setup and is described
in Sec. \ref{ch4:DevelopmentSetup}.

The library provides a script \emph{pylibxai\_test.sh} as shown in Appendix. \ref{appendix:TestingListing} that
executes both pytest's unit tests and also a functional tests
that utilizes the framework's explanation runner script \textbf{pylibxai\_explain.py} to
test functionally the entire process. The script may be launched from the command-line:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{text}
$ ./pylibxai_test.sh
\end{minted}

The shortened testing results are visible in Fig. \ref{fig:TestingScriptOutput}.
The script output displays launched tests where all 73 unit tests along with functional
tests performing explanation passed correctly.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\scriptsize, breaklines, xleftmargin=12pt, linenos, mathescape]{text}
$ ./pylibxai_test.sh
======================================================================== test session starts =========================================================================
platform win32 -- Python 3.9.20, pytest-8.4.1, pluggy-1.6.0
rootdir: C:\Users\mfalk\Desktop\pylibxai
configfile: pyproject.toml
plugins: anyio-3.7.1, typeguard-4.3.0
collected 73 items

pylibxai\pylibxai_context\test_pylibxai_context.py .................                                                                                            [ 23%]
pylibxai\Interfaces\test_interfaces.py .................                                                                                                        [ 46%]
pylibxai\Explainers\test_explainers.py ....................                                                                                                     [ 73%]
pylibxai\Views\test_web_view.py ...................                                                                                                             [100%]

========================================================================== warnings summary ==========================================================================
..\..\anaconda3\envs\pylibxai_env2\lib\site-packages\tensorflow\python\framework\dtypes.py:205
  tensorflow\python\framework\dtypes.py:205: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
    np.bool8: (False, True),

..\..\anaconda3\envs\pylibxai_env2\lib\site-packages\flatbuffers\compat.py:19
  flatbuffers\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
    import imp

..\..\anaconda3\envs\pylibxai_env2\lib\site-packages\tensorboard\compat\
tensorflow_stub\dtypes.py:326
  tensorboard\compat\tensorflow_stub\dtypes.py:326: DeprecationWarning: `np.bool8`
  is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
    np.bool8: (False, True),

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=================================================================== 73 passed, 3 warnings in 7.91s ===================================================================
[TEST1] CNN14, LIME, Integrated Gradients, Sandman 5s
[TEST2] HarmonicCNN, LIME, Integrated Gradients, Sandman 5s
[TEST3] GtzanCNN, Integrated Gradients, LRP
\end{minted}
\caption{The output of the testing script \emph{pylibxai\_test.sh} showing all tests passing.}
\label{fig:TestingScriptOutput}
\end{figure}

\subsection{Framework's unit tests}

The unit tests were implemented using \emph{pytest} tool and
their include total of 73 tests. The implemented tests may be grouped by their
testing component and summarized, including:

\begin{itemize}
    \item \textbf{Explainers tests} - The scope of unit testing of the explainer classes includes
        tests for type checking, input parameter edge cases, and generated explanation results tests.
        An exemplary test is shown in Fig. \ref{fig:ExplainerUnitTest} which tests whether
        the \texttt{LimeExplainer} classes properly return an error when the adapter class passed as
        input does not implement the expected \texttt{LimeAdapter}.
    
\begin{figure}[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
def test_lime_explainer_with_non_lime_adapter_raises_error(self):
    """Test LimeExplainer rejects adapters that don't implement LimeAdapter"""
    class NonLimeAdapter:
        def some_method(self):
            pass
    context = Mock()
    adapter = NonLimeAdapter()
    with pytest.raises(TypeError) as excinfo:
        LimeExplainer(adapter, context, ViewType.DEBUG)
    
    assert "LimeExplainer must be initialized with a model adapter that implements LimeAdapter interface" in str(excinfo.value)
\end{minted}
\caption{The unit test checks if the adapter class passed to the explainer class has the \texttt{LimeAdapter} interface.}
\label{fig:ExplainerUnitTest}
\end{figure}

    \item \textbf{Models tests} - The testing of the integrated models is performed on the integration level with
        functional tests that are part of the \emph{pylibxai\_test.sh} script, as shown in Appendix
        \ref{appendix:TestingListing}. The exemplary test that invokes an explanation of the
        \emph{GtzanCNN} model using Integrated gradients and LRP methods is shown in Fig.
        \ref{fig:ModelIntegrationTest}.

\begin{figure}[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{text}
pylibxai_explain.py -w ./gtzancnn_expl/ -m GtzanCNN --explainer=integrated-gradients,lrp --target=jazz -i ./data/gtzan_jazz.wav
\end{minted}
\caption{The integration tests of the \emph{GtzanCNN} model using a jazz song from \emph{GTZAN} dataset.}
\label{fig:ModelIntegrationTest}
\end{figure}

    \item \textbf{Interfaces tests} - The scope of unit testing of the interface classes includes tests for
        proper interface implementation, partial implementation detection, multi-interface support, and inheritance
        chain contract maintenance. The exemplary tests for interface classes that check if a partial implementation
        of the \texttt{ModelLabelProvider} class raises an error are shown in Fig. \ref{fig:InterfaceUnitTest}.

\begin{figure}[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
def test_model_label_provider_partial_implementation_raises_error(self):
    """Test that ModelLabelProvider raises TypeError when only one abstract method is implemented"""
    class PartialModelLabelProvider(ModelLabelProvider):
        def get_label_mapping(self) -> Dict[int, str]:
            return {0: "rock", 1: "pop", 2: "jazz"}
        # Missing map_target_to_id
    with pytest.raises(TypeError) as excinfo:
        PartialModelLabelProvider()
    
    assert "abstract method" in str(excinfo.value).lower()
    assert "map_target_to_id" in str(excinfo.value)
\end{minted}
\caption{The unit test that checks if partial implementation of the \texttt{ModelLabelProvider} interface raises an error.}
\label{fig:InterfaceUnitTest}
\end{figure}

    \item \textbf{Context tests} - The scope of unit testing of the interface classes includes tests for directory
        management and creation, plot generation, attribution data JSON serialization, and audio file operations.
        The exemplary tests for interface classes that test JSON serialization of explanation’s attribution are
        shown in Fig. \ref{fig:ContextUnitTest}.

\begin{figure}[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
def test_write_attribution_creates_json(self, context, temp_dir):
    """Test attribution JSON creation"""
    test_data = np.array([0.1, 0.2, 0.3, 0.4])
    suffix = "test_attribution.json"
    context.write_attribution(test_data, suffix)
    file_path = os.path.join(temp_dir, suffix)
    assert os.path.exists(file_path)
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    assert "attributions" in data
    assert data["attributions"] == [0.1, 0.2, 0.3, 0.4]
\end{minted}
\caption{}
\label{fig:ContextUnitTest}
\end{figure}

    \item \textbf{Views tests} - The scope of unit testing of the interface classes includes tests for server
        startup and process management, error handling for subprocess failures, and resource cleanup and shutdown
        procedures. The exemplary test for interface classes that test frontend server initialization
        is shown in Fig. \ref{fig:ViewUnitTest}.

\begin{figure}[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
@patch('pylibxai.Views.web_view.subprocess.Popen')
@patch('pylibxai.Views.web_view.run_file_server')
@patch('pylibxai.Views.web_view.os.environ')
def test_start_success(self, mock_environ, mock_run_file_server, mock_popen, web_view):
    """Test successful start of WebView."""
    # ...
    mock_env = {'EXISTING_VAR': 'value'}
    mock_environ.copy.return_value = mock_env
    web_view.start()
    
    mock_run_file_server.assert_called_once_with("/test/workdir", 8000)
    assert web_view.server == mock_server
    assert mock_env['VITE_PYLIBXAI_STATIC_PORT'] == '8000'
    mock_popen.assert_called_once_with(
        ['npm', 'run', 'dev'],
        cwd=web_view.vite_dir, env=mock_env,
        stdout=sys.stdout, stderr=sys.stderr, shell=True
    )
    
    assert web_view.vite_process == mock_process
\end{minted}
\caption{}
\label{fig:ViewUnitTest}
\end{figure}
\end{itemize}

% #############################################
%
% Rozdział 7 - Podsumowanie
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Conclusions} \label{ch:summary}

Solving this issue is a intresting/promising idea for a future work.

Wnioski:
- W przypadku niektórych metod wymagających obliczeń per warstwę modelu jak LRP,
  wymagane jest zadefiniowanie handlera per-klasę co powoduje, że modele audio
  nie działają. Fakt ten uniemożliwił w bibliotece zaimplementowanie wsparcia
  LRP dla HCNN oraz CNN14, które to posiadają własne warstwy

biblioteka captum

%---------------
% Bibliografia
%---------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\printbibliography
\clearpage

% Wykaz symboli i skrótów.
% Pamiętaj, żeby posortować symbole alfabetycznie
% we własnym zakresie. Makro \acronymlist
% generuje właściwy tytuł sekcji, w zależności od języka.
% Makro \acronym dodaje skrót/symbol do listy,
% zapewniając podstawowe formatowanie.
\acronymlist
\acronym{XAI}{Explainable artificial intelligence}
\acronym{SOTA}{State-of-art Music Models}
\acronym{LRP}{Layer-wise relevance propagation}
\acronym{JSON}{JavaScript Object Notation}
\acronym{CORS}{Cross-origin resource sharing}
\acronym{HTTP}{Hypertext Transfer Protocol}
\acronym{API}{Application Programming Interface}
\vspace{0.8cm}

%--------------------------------------
% Spisy: rysunków, tabel, załączników
%--------------------------------------
\pagestyle{plain}

\listoffigurestoc    % Spis rysunków.
\vspace{1cm}         % vertical space
\listoftablestoc     % Spis tabel.
\vspace{1cm}         % vertical space
\listofappendicestoc % Spis załączników

%-------------
% Załączniki
%-------------

% Obrazki i tabele w załącznikach nie trafiają do spisów
\captionsetup[figure]{list=no}
\captionsetup[table]{list=no}

% Załącznik 1
\clearpage

\appendix{Listing of the script \texttt{pylibxai\_explain.py}} \label{appendix:ScriptListing}
\begin{minted}[numbersep=12pt, fontsize=\scriptsize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
import torch
import argparse
import torchaudio
import os

from pylibxai.model_adapters import HarmonicCNN, Cnn14Adapter, GtzanCNNAdapter
from pylibxai.pylibxai_context import PylibxaiContext
from pylibxai.Explainers import LimeExplainer, IGradientsExplainer, LRPExplainer
from pylibxai.Interfaces import ViewType, ModelLabelProvider
from utils import get_install_path

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
GTZAN_MODEL_PATH= get_install_path() / "pylibxai" / "models" / "GtzanCNN" / "gtzan_cnn.ckpt"

def main():
    parser = argparse.ArgumentParser(description="Process a model name and input path.")
    
    parser.add_argument('-m', '--model', type=str, required=True,
                        help="Name of the model to use [{sota_music, paans, gtzan},]...")
    parser.add_argument('-u', '--visualize', action='store_true',
                        help="Enable visualization of audio in browser-based UI.")
    parser.add_argument('-e', '--explainer', type=str, required=True,
                        help="Name of the explainer to use [lime, integrated-gradients, lrp].")
    parser.add_argument('-t', '--target', type=str, required=True,
                        help="Name or index of the label to explain.\
                              Mapping is done automatically based on the model if the model provides it.") 
    parser.add_argument('-i', '--input', type=str, required=True,
                        help="Path to the input file or directory.") 
    parser.add_argument('-w', '--workdir', type=str, required=True,
                        help="Path to the workdir directory.")
    parser.add_argument('-d', '--device', type=str, default=DEVICE,
                        help="Device to use for computation [cpu, cuda]. Default is 'cuda' if available, otherwise 'cpu'.")
    args = parser.parse_args()
    
    device = args.device if args.device is not None else DEVICE
    assert device in ['cpu', 'cuda'], "Device must be either 'cpu' or 'cuda'."
    
    expls = args.explainer.split(",")
    assert all(ex in ["lime", "integrated-gradients", "lrp"] for ex in expls), \
        "Invalid explainer specified. Available options: [lime, integrated-gradients, lrp]."

    context = PylibxaiContext(args.workdir)

    if args.model == "HCNN":
        adapter = HarmonicCNN(device=device)
    elif args.model == "CNN14":
        adapter = Cnn14Adapter(device=device)
    elif args.model == "GtzanCNN":
        adapter = GtzanCNNAdapter(model_path=GTZAN_MODEL_PATH, device=device)
    else:
        print('Invalid value for -m/--model argument, available: [HCNN, CNN14, GtzanCNN].')
        return
    
    view_type = ViewType.WEBVIEW if args.visualize else ViewType.DEBUG
    expl_count = len(expls)

    # Attempt parsing label as an integer, if it fails then assume it's a string label
    try:
\end{minted}
\begin{minted}[numbersep=12pt, firstnumber=59, fontsize=\scriptsize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
        target = int(args.target)
    except ValueError:
        target = args.target

    # copy input audio to workdir
    context.write_audio(args.input, os.path.join("input.wav"))
    if issubclass(type(adapter), ModelLabelProvider):
        context.write_label_mapping(adapter.get_label_mapping(), os.path.join("labels.json"))
    
    if "lime" in expls:
        view = view_type if expl_count == 1 else ViewType.NONE
        expl_count -= 1
        explainer = LimeExplainer(adapter, context, view_type=view)
        explainer.explain(args.input, target=None)
    if "lrp" in expls:
        view = view_type if expl_count == 1 else ViewType.NONE
        expl_count -= 1
        audio, _ = torchaudio.load(args.input, normalize=True)
        audio = audio.to(device)
        explainer = LRPExplainer(adapter, context, device, view_type=view)
        explainer.explain(audio, target=target)
    if "integrated-gradients" in expls:
        view = view_type if expl_count == 1 else ViewType.NONE
        expl_count -= 1
        audio, _ = torchaudio.load(args.input, normalize=True)
        audio = audio.to(device)
        explainer = IGradientsExplainer(adapter, context, device, view_type=view)
        explainer.explain(audio, target=target)

if __name__ == '__main__':
    main()
\end{minted}

% Załącznik 2
\clearpage
\appendix{The testing script \emph{pylibxai\_test.sh}} \label{appendix:TestingListing}

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{bash}
#!/usr/bin/env bash

GREEN="\033[32m"
CLR="\033[0m"

# Run unit tests for pylibxai
python -m pytest -v pylibxai/pylibxai_context/test_pylibxai_context.py \
                    pylibxai/Interfaces/test_interfaces.py \
                    pylibxai/Explainers/test_explainers.py \
                    pylibxai/Views/test_web_view.py


echo -e "${GREEN}[TEST1]${CLR} CNN14, LIME, Integrated Gradients, Sandman 5s"
mkdir -p ./cnn14_expl/ &&
python ./pylibxai/pylibxai_explain.py -w ./cnn14_expl/ -m CNN14 --explainer=lime,integrated-gradients --target=0 -i ./data/sandman_5s.wav &&
rm -rf ./cnn14_expl/

echo -e "${GREEN}[TEST2]${CLR} HarmonicCNN, LIME, Integrated Gradients, Sandman 5s"
mkdir -p ./harmoniccnn_expl/ &&
python ./pylibxai/pylibxai_explain.py -w ./harmoniccnn_expl/ -m HCNN --explainer=lime,integrated-gradients --target=0 \
                           -i ./data/sandman_5s.wav &&
rm -rf ./harmoniccnn_expl/

echo -e "${GREEN}[TEST3]${CLR} GtzanCNN, Integrated Gradients, LRP"
mkdir -p ./gtzancnn_expl/ &&
python ./pylibxai/pylibxai_explain.py -w ./gtzancnn_expl/ -m GtzanCNN --explainer=integrated-gradients,lrp --target=jazz \
                           -i ./data/gtzan_jazz.wav &&
rm -rf ./gtzancnn_expl/
\end{minted}

\end{document} % Dobranoc.
