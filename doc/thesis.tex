%-----------------------------------------------
%  Engineer's & Master's Thesis Template
%  Copyleft by Artur M. Brodzki & Piotr Woźniak
%  Warsaw University of Technology, 2019-2022
%-----------------------------------------------

\documentclass[
    bindingoffset=5mm,  % Binding offset
    footnoteindent=3mm, % Footnote indent
    hyphenation=true    % Hyphenation turn on/off
]{src/wut-thesis}

%\usepackage{setspace}
%\onehalfspacing
%\frenchspacing

\graphicspath{{tex/img/}} % Katalog z obrazkami.

%\usepackage[style=authoryear]{biblatex}
\addbibresource{bibliografia.bib} % Plik .bib z bibliografią

% Do debugowania czy tekst wychodzi poza margines
%\usepackage{showframe}

\usepackage{minted}
\usepackage{mdframed}
\usepackage{float}
\newtheorem{example}{Example}

\mdfdefinestyle{mintedframe}{
    innertopmargin=1mm,
    frametitlebelowskip=0pt,
    frametitleaboveskip=0pt,
    splittopskip=0pt,
    linewidth=0.75pt
}
\surroundwithmdframed[style=mintedframe]{minted}

\mdfdefinestyle{verbatimframe}{
    innertopmargin=1mm,
    frametitlebelowskip=0pt,
    frametitleaboveskip=0pt,
    splittopskip=0pt,
    linewidth=0.75pt
}
\surroundwithmdframed[style=verbatimframe]{verbatim}

%-------------------------------------------------------------
% Wybór wydziału:
%  \facultyeiti: Wydział Elektroniki i Technik Informacyjnych
%  \facultymeil: Wydział Mechaniczny Energetyki i Lotnictwa
% --
% Rodzaj pracy: \EngineerThesis, \MasterThesis
% --
% Wybór języka: \langpol, \langeng
%-------------------------------------------------------------
\facultyeiti    % Wydział Elektroniki i Technik Informacyjnych
\MasterThesis % Praca inżynierska
\langeng % Praca w języku polskim

\begin{document}

%------------------
% Strona tytułowa
%------------------
\instytut{Control and Computation Engineering}
\kierunek{computer science}
\specjalnosc{Internet Management Support Systems}
\title{
    Machine learning framework for explainable artificial intelligence models
}
% Title in English for English theses
% In English theses, you may remove this command
\engtitle{
    Machine learning framework for explainable artificial intelligence models
}
% Title in Polish for English theses
% Use it only in English theses
\poltitle{
    Machine learning framework for explainable artificial intelligence models
}
\author{Maciej Falkowski, BSc}
\album{329117}
\promotor{Mateusz Modrzejewski, PhD}
\date{\the\year}
\maketitle

%-------------------------------------
% Streszczenie po polsku dla \langpol
% English abstract if \langeng is set
%-------------------------------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\abstract
The aim of this thesis is to present a system that aims to simplify the analysis of explainable artificial
intelligence (XAI) techniques in audio machine learning models.

The proposed system is a framework that integrates selected XAI techniques such as
audioLIME, SHAP (Shapley Additive explanations), and Layer-wise Relevance Propagation (LRP). The framework aims
to provide a unified and extensible infrastructure for creating, managing, and visualizing model explanations.

The framework offers an application programming interface
in a form of a Python library and supporting tools that will allow its user
to integrate their model into the system and launch different XAI techniques on their model.
It is structured using the Model-View-Controller (MVC) design pattern, ensuring modularity,
scalability, and separation of concerns. The Model layer encapsulates operations and the machine learning
models.

The View layer defines an intuitive user interface tailored to data scientists and machine learning engineers, enabling interactive exploration of explanation outputs; and the Controller layer manages communication between the model and interface components.

% Ważne by napisać czym praca nie jesy
The aim of the thesis is not to perform any research about integrated machine learning models and explanation
techniques. The thesis does not evaluate any of their metrics such as performance or accuracy, nor it compares them one to another.

A key contribution of this work is the formalization of design principles and integration strategies that support multimodal explanations, including both visual and auditory outputs, with a focus on interpretability and usability.

The framework includes visualization modules for displaying saliency maps, relevance heatmaps, and feature contribution plots, dynamically generated based on user queries and model outputs. In addition, user roles and interaction paradigms are defined to accommodate different levels of expertise, ensuring accessibility for both novice and expert users. The evaluation of the system demonstrates its effectiveness in improving interpretability and user understanding of complex model behaviors.

This work lays the groundwork for future research and development in human-centric AI systems, offering a flexible foundation for building transparent and trustworthy machine learning applications.
\keywords XXX, XXX, XXX

%----------------------------------------
% Streszczenie po angielsku dla \langpol
% Polish abstract if \langeng is set
%----------------------------------------
\clearpage
\secondabstract \kant[1-3]
\secondkeywords XXX, XXX, XXX

\pagestyle{plain}

%--------------
% Spis treści
%--------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\tableofcontents

%------------
% Rozdziały
%------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\pagestyle{headings}

% #############################################
%
% Rozdział 1 - Introduction
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Introduction} \label{ch:introduction}

% Akapit z cytatem
\lipsum[1] \cite{goossens93}

% Przykładowy obrazek
\begin{figure}%[!h]
    % Wyrównanie obrazka, szerokość i plik
    % Zamiast width można też użyć height, etc.
    \centering \includegraphics[width=0.5\linewidth]{logopw.png}
    % Podpis umieszczamy pod obrazkiem
    % znacznik \caption służy również do wygenerowania numeru obrazka
    \caption{Tradycyjne godło Politechniki Warszawskiej}
    % \label pozwala odwołać się do obrazka w innych miejscach za pomocą \ref
    % odwołanie \ref renderuje się jako numer obrazka,
    % dlatego zawsze najpierw używaj \caption a potem \label
    \label{fig:tradycyjne-logo-pw}
\end{figure}

% \ref wyrenderuje się jako 'Reference to image 1.1'
\lipsum[2] Reference to image \ref{fig:tradycyjne-logo-pw}.

% Lista punktowana
% Parametr label ustawia symbol punktora
\begin{itemize}
    \item Item 1:
    \begin{itemize}[label=---]
        \item item 1.1;
        \item item 1.2;
    \end{itemize}
    \item Item 2;
    \item Item 3.
\end{itemize}

\lipsum[3]

% Lista numerowana w formacie 1.a).ii
% Tutaj również można stosować \label
\begin{enumerate}
    \item Item 1:
    \begin{enumerate}
        \item item 1.1;
        \item item 1.2:
        \begin{enumerate}
            \item item 1.2.1;
            \item item 1.2.2;
        \end{enumerate}
        \item item 1.3;
    \end{enumerate}
    \item Item 2;
    \item Item 3.
\end{enumerate}

% Przypis dolny \footnote
\lipsum[4] Lorem ipsum dolor sit amet\footnote{Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.}, consectetur adipiscing elit.

% Przykładowa tabela: wyśrodkowana i renderowana
% w miejscu wstawienia: !h = !h[ere]
% Domyślnie tabele trafiają na górę strony
\begin{table}[!h] \centering
    % Podpis tabeli umieszczamy od góry
    \caption{Przykładowa tabela.}
    \label{tab:tabela1}

    % Tabela z trzema kolumnami:
    % dwie wyrównanie do środka [c], a ostatnia do prawej [r]
    % szerokość kolumn automatyczna (równa szerokości tekstu)
    \begin{tabular}{| c | c | r |} \hline
        Kolumna 1       & Kolumna 2 & Liczba \\ \hline\hline
        cell1           & cell2     & 60     \\ \hline
        cell4           & cell5     & 43     \\ \hline
        cell7           & cell8     & 20,45  \\ \hline
        % Komórka o szerokości dwóch kolumn, wyrównana do prawej
        % Przypisy dolne w tabelach wstawiamy przez \tablefootnote
        \multicolumn{2}{|r|}{Suma\tablefootnote{Table footnote.}} & 123,45 \\ \hline
    \end{tabular}

\end{table}

Lorem ipsum dolor sit amet.

% #############################################
%
% Rozdział 2 - Analiza problemu
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Problem analysis} \label{ch1:probAnalysis}

This chapter provides an overview of terminology and methods used in the thesis,
which describes essentials of machine learning explanations, description of explanation methods used in the thesis which are audioLIME, LRP, SHAP, and software engineering principles used for designing a framework for audio machine learning explanations.

\subsection{Black box interpretation of ML predictors} % Why explanation framework for audio - the problem
\subsubsection{Why explanation framework for audio - the problem}

\subsection{Similar solutions}

To the best of our knowledge this is the ...

\subsection{Selection of methodology - overview of framework, monolithic UI etc.}

\subsection{Framework definition}
\subsubsection{Definition}
\subsubsection{Modularity}
\subsubsection{Dependency injection}

\subsection{Definition of user}

\subsection{Definition of ML explanation}
\subsubsection{Locality-awareness}
\subsubsection{Model-agnostic}

\subsection{Audio processing}
\subsubsection{Waveform representation}
\subsubsection{Mel-spectogram}

\subsection{Model explanation}
\subsubsection{definition}
\subsubsection{Locality-awareness}
\subsection{Attribution intensity} \label{ch2:AttrIntensity}

\subsection{Selection of explanation methods for framework}
\subsubsection{Criteria - interpretability, attribution, model-agnostic}
\subsubsection{AudioLIME} \label{ch1:audioLIME}
\subsubsection{Shapley Additive Explanations} \label{ch1:ShapMethod}
\subsubsection{Layer-wise Relevance Propagation} \label{ch1:LrpMethod}

\subsection{Selection of model for integration} \label{ch1:ModelSelection}
\subsubsection{Criteria - practicality}
\subsubsection{HarmonicCNN}
HarmonicCNN - a model from a repository \textit{State-of-the-art Music Tagging Models} \cite{Won2020-ej}.
\subsubsection{CNN14}
\subsubsection{Handmade Gtzan / GtzanCNN}

\subsection{ML Explanation framework overview} \label{ch2:ArchitectureOverview}
\subsubsection{Framework components - library, UI, terminal command}

\subsection{Visualizing ML Explanation framework overview} 

\subsection{Model-View-Controller} \label{ch1:ModelViewController}
\subsubsection{Architecture of Model-View Controller}
\subsubsection{Characteristic of Model, View and Controller}
\subsubsection{Separation of operations on data and visualization}

% #############################################
%
% Rozdział 3 - Requirements and tools (może Framework requirements?)
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Requirements and tools} \label{ch:reqrTools}

        This chapter defines and formalizes requirements that the project must actualize
    to meet the objective of the thesis, including software and behavioral requirements.
    The chapter starts with a definition of functional and non-functional requirements,
    followed by an overview of software requirements and tools used in the thesis.

\subsection{Functional requirements}

    The functional requirements seek to define how a framework should behave to its user.
    For clarity the requirements are divided based on the software component of the framework which
    in framework's case are library, user web interface, and terminal helper tool.

% Wymagania interfejsu, backend serwera, UI
\subsubsection{Library application programming interface requirements}
    \begin{enumerate}[itemsep=1\baselineskip]

    \item \textbf{Provide user model injection}

        The library API must enable users to create adapters for their classes allowing them to inject operations on
    their data into the framework. Users may implement custom model adapters by extending the provided base
    classes. This enables integration of various audio model architectures with the explanation
    framework.

    \item \textbf{Provide explanation interface for model adapters}

        The library API must offer base classes for implementing a given explanation method.
    Each adapter type must define specific methods required for the corresponding explanation algorithm,
    including LIME, SHAP, or LRP, ensuring a consistent interface for different explanation methods.

    \item \textbf{Explanation method Selection}

        The library API must offer users the ability to choose which explanation methods are displayed in the view.
    Users can select from available explainers including LIME, SHAP, and LRP through library API or through
    command-line arguments in case when using a helper tool. This allows users to focus on specific explanation
    approaches relevant to their use case.

    \item \textbf{Support necessary audio format}

        The library API must support user audio input provided in \texttt{.wav} and \texttt{.mp4} formats.
    Audio should be automatically converted to appropriate tensor formats for model processing.

    \item \textbf{Provide interface for view classes}

        The library API must offer a base interface for implementing views which will fulfill
    the requirement of modularity and allow a system to support multiple view types.

    \item \textbf{Context Management}

        The library API must define a context that serves to store and save the current explanation progress.
    The context class manages working directories, file operations, and explanation state throughout the process.
    Context provides centralized access to resources and maintains consistency across different explanation phases.

    \item \textbf{Interface for prediction class information}

        The library should offer an interface enabling users to inject prediction class information.
    This information is essential for proper explanation interpretation and visualization.
    The system should support label retrieval dynamically depending on whether the user provided
    support for their user-defined model adapters.

    \item \textbf{Support for selected explanation methods}

        The library API must provide explanation methods selected for the thesis, such as LIME, SHAP, and LRP.
    These methods should be implemented as classes, and they also serve as controllers within
    the Model-View-Controller pattern and coordinate between model adapters, explanation algorithms, and view components. They encapsulate the complete explanation workflow from input processing to result visualization.

    \item \textbf{Support for web interface view}

        The library API must define web server view classes that manage file serving and web-based user interface.
    Web server components handle HTTP requests and serve explanation results through a user interface.

    \item \textbf{Ensure persistent result storage}

        The library API must ensure persistent storage of user results on disk to prevent
    data loss. The context system should provide this by saving explanation artifacts to
    the working directory. This will enable users to archive explanation outputs across 
    multiple sessions.
    
    \end{enumerate}

\subsubsection{User web interface requirements}
    \begin{enumerate}[itemsep=1\baselineskip]
    %\vspace{\baselineskip}
    \item \textbf{Separate tab for each explanation result}

            The user interface must aggregate the results of each explanation method in separate tabs to organize
        outcomes and enable switching between them. Each explanation method, which is LIME, SHAP, or LRP,
        should get its dedicated tab. This should allow users to easily navigate different explanation approaches
        to compare results. The sidebar should provide a structured view of all generated explanations in a single
        interface and allow you to switch between them.

    \item \textbf{Display original audio playback}

        The user interface must display the original audio track that serves as input for explanations across all
    explanation methods. The interface must enable audio playback functionality, including play/pause controls and
    waveform visualization. Users can listen to the original audio while examining explanation results for better
    context understanding. The waveform display provides a visual representation of the audio signal structure.

    \item \textbf{Visualize LIME explanation result}

        The user interface must display LIME explanation result as playable audio.
    Users should be able to listen to specific audio segments that contributed most significantly
    to the model's prediction. This enables an intuitive understanding of which temporal parts of
    the audio influenced the model's decision. The width of the explained audio visualization
    should be aligned with the original audio.

    \item \textbf{Provide version information}

        The user interface must display information about the library version so that users may verify which
    version of a library they are currently using. Such information may prove to be useful,
    for example, for troubleshooting.

    \item \textbf{Display notification and error information}

        The user interface must display notifications about errors or other information in a dedicated information
    panel. The notification system provides user feedback for processing status, errors, warnings,
    and completion messages. Users receive clear communication about the system state and any issues that
    require attention. The dedicated panel ensures important messages are prominently displayed without
    disrupting the main workflow.

    \item \textbf{Visualize attribution intensity}

        The user interface must show plot of attribution intensity of user's input audio
    for both SHAP and LIME explanations. The attribution scores given to various audio input segments are displayed in these plots. This enables users to determine which audio signal regions had the biggest impact on the model's prediction.

    \item \textbf{Display of user's input overlayed with attribution}

        The user interface must display attribution overlaid on user's input for both SHAP and LIME explanations.
    Attribution values are visualized as color heatmaps on the user's spectrogram which eDables a user to
    understand which frequency components at a given time interval influenced the model's decision.
    This method works very well when the input provided to the model is a spectrogram.

    \item \textbf{Display prediction classes of user's model}

        The user interface must display optional model label descriptions when provided by the user's model adapter.
    The interpretability of explanation visualizations is improved by label information.

    \end{enumerate}

\subsubsection{Terminal application requirements}
    Framework must provide a terminal-application tool for launching supported models on framework's explanation method with input provided by the user. The tool then will implement a typical usage scenario:
    \begin{enumerate}
        \item User input is converted from a waveform input into model's type which is either tensor of input samples or spectrogram type.
        \item The tool defines a necessary model adapter,
    \end{enumerate}

    \begin{enumerate}[itemsep=1\baselineskip]

    \item \textbf{Provide selection of one or more explanation method}

        The application must enable users to select one or more explanation methods offered by the framework which are LIME, SHAP, or LRP.

    \item \textbf{Provide selection of audio model}

            The application must enable users to select from models implemented within the framework that they
        intend to use. 

    \item \textbf{Provide selection of visualization method}

        The application must enable users to select the visualization method implemented in the framework.
    Users can choose between web-based visualization and disabled visualization.

    \item \textbf{Provide selection of working directory}

    The application must enable users to select a folder as the working directory or create a default when not provided by the user.

    \end{enumerate}

\subsection{Non-functional requirements} \label{ch:NonFuncRequirements}

    The implementation of an audio XAI framework must meet several criteria
    top ensure proper behavior for the user and quality criteria.
    The most important criteria are modularity, usability, and persistency.

\begin{description}
    \item \textbf{Modularity}

         The project thesis aims to provide a general and extensible audio explanation framework. The system must be 
    designed with a modular structure, where key components such as explanation methods, model adapters, and view renderers
    are implemented as independent modules. For that, the system must adhere to the pattern principles of Model-View-
    Controller architecture as discussed in <Sec. TODO> This modularity allows for the seamless integration of new 
    functionalities, such as adding additional explanation algorithms or supporting new audio models, without requiring 
    modifications to the core of the system.

    \item \textbf{Reliability}

        The system must behave stably and predictably during normal operation. During regular operation, the system has to
    display stable and predictable behavior. The user interface, audio processing pipelines, and explanation methods must
    all gracefully handle runtime errors and invalid inputs. Errors must be properly logged, and informative feedback must
    be sent to the user without crashing. To avoid data loss, intermediate and final outputs need to be saved consistently.

    \item \textbf{Usability}

        The framework library and web user interface must follow intuitive design principles, including consistent
    navigation, clear labeling, and visual clarity of explanation results. The use of separate tabs for different 
    explanation methods ensures that users can focus on one explanation at a time while easily switching between views. 
    Notifications and status indicators must be visible and should inform a user about the behavior of the interface. In 
    addition, the command-line application helper tool must be a fast way of testing the library implementation and environment setup.

\end{description}

\subsection{Tools specification} \label{ch:ch3LabelSpec}
\begin{description}
\item \textbf{PyTorch}

A comprehensive open-source machine learning framework that provides exceptional flexibility for building and training neural networks with dynamic computational graphs. PyTorch offers seamless GPU acceleration capabilities enabling efficient training of complex deep learning models on large datasets. The framework provides extensive libraries of pre-built components and optimizers that accelerate development while maintaining a pythonic interface. PyTorch has become the preferred framework for research due to its intuitive debugging experience and strong community support.

\item \textbf{librosa}

A specialized Python library for music and audio signal processing that provides comprehensive tools for analyzing sound data through various time-frequency transforms. Librosa offers advanced capabilities for rhythm analysis, beat detection, and feature extraction of key acoustic characteristics commonly used in machine learning pipelines. The library integrates smoothly with numerical computing libraries while supporting various audio formats with built-in resampling capabilities. Librosa emphasizes reproducible research with transparent implementations of audio algorithms complemented by excellent documentation.

\item \textbf{torchaudio}

A domain-specific library built on PyTorch that provides GPU-accelerated tools for audio processing tasks in deep learning applications. TorchAudio offers seamless integration with PyTorch's autograd system, enabling gradient-based training with audio feature extractors as differentiable components. The library includes specialized audio augmentation techniques and pre-trained models for common audio tasks with a consistent tensor-based API. TorchAudio ensures compatibility with PyTorch's ecosystem and is actively maintained with regular updates incorporating state-of-the-art techniques.

\item \textbf{pytest}

A sophisticated testing framework for Python that emphasizes simplicity with a minimalist syntax for writing tests and a powerful fixture system for managing dependencies. Pytest's parameterization features enable running tests with multiple inputs while providing detailed assertion introspection to quickly identify issues. The framework's plugin architecture has fostered a rich ecosystem of extensions for specialized testing needs including coverage analysis. Pytest supports both unit and functional testing with automatic test discovery and comprehensive reporting features.

\item \textbf{matplotlib}

A comprehensive data visualization library for Python that has become the standard for creating publication-quality figures and interactive visualizations. Matplotlib offers exceptional flexibility through its object-oriented API, allowing precise control over every aspect of plots for creating complex custom visualizations. The library supports an extensive range of plot types from basic line graphs to specialized visualizations suitable for diverse data representation needs. Matplotlib integrates seamlessly with NumPy and pandas while providing robust support for mathematical expressions through LaTeX rendering.

\item \textbf{captum}

An interpretability library built specifically for PyTorch that implements state-of-the-art algorithms for explaining decisions made by deep learning models. Captum provides unified implementations of gradient-based attribution methods that reveal feature importance across various model architectures at multiple granularities. The library implements advanced visualization techniques for different data modalities, rendering attributions as heatmaps for images and waveforms for audio. Captum's attribution methods support batch processing and GPU acceleration with a consistent API that works across different model architectures.

\item \textbf{React}

A declarative JavaScript library that has revolutionized user interface development through its component-based architecture and efficient virtual DOM rendering system. React promotes unidirectional data flow that makes application state changes predictable while its JSX syntax combines JavaScript with HTML-like templates for creating reusable components. The library's robust ecosystem includes state management solutions, routing libraries, and testing frameworks that address common development challenges. React's composition-focused design encourages creation of small, focused components that can be combined to build complex interfaces while promoting code reuse.
\end{description}

% #############################################
%
% Rozdział 4 - External specification
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{External specification} \label{ch:externalSpec}

\subsection{Installation process overview}
\subsubsection{Installation steps}

\subsection{Command-line tool for simple explanations} \label{ch4:CmdTool}

\subsection{Web interface for explanation}
\subsubsection{Initalizing web interface}
\subsubsection{Displaying LIME results}
\subsubsection{Displaying LRP results}
\subsubsection{Displaying SHAP results}
\subsubsection{Displaying software warning} \label{ch4:NotificationPanel}
\subsubsection{Version information}

% #############################################
%
% Rozdział 5 - Framework implementation
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Framework implementation} \label{ch:implementation}

    The following chapter contains an overview of the architecture and implementation details of the explanation framework. It briefly discusses each of the system's components by providing
    a walk-through of each step of the framework's explanation pipeline.

\subsection{Software architecture overview}

    The architecture of the project consists of three main components that are library, web interface, and command-line helper script.
    These component implement all steps of a full-pipeline from preprocessing
    user's input up to visualizing explanation results as discussed in Sec. \ref{ch2:ArchitectureOverview}.

\begin{enumerate}
    \item \textbf{Library} - Provides the main logic and implementation. It is interfacing directly with a user and
    provides the necessary tools for the execution of an explanation. This includes interfaces for the creation of 
    adapters for users' models, logic implementing supported explanation types, visualization of explanations that 
    include a frontend web interface, and helper utilities.

    \item \textbf{Web interface} - Defines a main visual view for the observation of the explanations' results. It is
    composed of two parts which the first is a backend server that serves the explanation results to the frontend part
    through the HTTP protocol. The frontend part then requests the required data from the backend server and visualizes
    it in the form of a browser page. The communication process between user application and framework is shown in
    \ref{fig:CommunicationArchitecture}.

    \item \textbf{Command-line explanation runner} - It is a toolchain's helper script and serves the purpose of quickly 
    launching a typical usage type of the library, which is loading an input audio file and then launching a chosen set 
    of explanations on the chosen library's provided model.
\end{enumerate}

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{comm.jpg}
    \caption{Communication scheme between user application and frontend.}
    \label{fig:CommunicationArchitecture}
\end{figure}

\subsection{Project structure}

    The project’s source code is grouped in multiple directories grouping similar implementations. They contain an implementation of each of the framework’s pipeline phases
    and unit test definitions for them.

\begin{itemize}[itemsep=1\baselineskip]
	\item \textbf{audioLIME/} - Contains implementation of audioLIME explanation method,
	\item \textbf{AudioLoader/} - Provides utilities for loading and preprocessing audio files,
	\item \textbf{Explainers/} - Contains explainer classes for LIME, SHAP, and LRP methods,
	\item \textbf{pylibxai\_explain.py} - Main command-line entry point for running explanations on audio models,
	\item \textbf{Interfaces/} - Defines abstract base classes and interfaces for model adapters and views,
	\item \textbf{model\_adapters/} - Implements adapter classes for integrating different audio models with the framework,
	\item \textbf{models/} - Contains pretrained model files and checkpoints for supported audio models,
	\item \textbf{pylibxai\_context/} - Implements context management for storing explanation state and working directories,
	\item \textbf{Views/} - Contains implementations of different views, such as web view for serving explanation results through HTTP to the web page, or debug view for quick debugging,
	\item \textbf{pylibxai-ui/} - Frontend React application providing web-based user interface for visualization,
\end{itemize}

\subsection{Model-View-Controller implementation overview}

\begin{figure}%[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{class_diagram.png}
    \caption{Library class diagram.}
    \label{fig:ClassDiagram}
\end{figure}

    The thesis utilizes Model-View-Controller (MVC) architectural pattern (Sec. \ref{ch1:ModelViewController}) to obtain extendable architecture that satisfies one of core requirements of the thesis that is modularity as discussed in Sec. \ref{ch:NonFuncRequirements}. This allows a framework to be easily extendable with a new explanation types, machine learning models, and views separating these three from each other so that the developer might work on one of these exclusively.

    The Model-View-Controller is implemented using
    the following classes visible in the class diagram \ref{fig:ClassDiagram}. The implementation and details of these classes are discussed later in the chapter:
    \begin{description}
        \item[Model] The function of the model in the framework is realized using adapter classes for user
        models, such as \texttt{HarmonicCNN} or \texttt{Cnn14} (Sec. \ref{ch1:ModelSelection}). These classes
        operate on user data passed to them from controller classes, performing data manipulation, which is an
        inference in this case. They need to be implemented by the user to provide their support to the framework and they usually come in a form of an adapter class.
        The interface required for implementation of these classes is discussed     
        in Sec. \ref{ch5:AdapterInterfaces},

        \item[View] The function of view in the framework is realized using classes such as \texttt{WebView}
        or \texttt{DebugView}. These classes receive explanation results specified in the \texttt{Context}
        class as required by the \texttt{ViewInterface} class and process them to display the results
        to the user,

        \item[Controller] The purpose of this part is to pass the user’s input into the model
        classes for data manipulation and to later pass the results of data manipulation into 
        the view. This functionality is achieved by classes implementing explainers' logic, 
        such as \texttt{LimeExplainer} or \texttt{ShapExplainer}.
    \end{description}

\subsection{Interfaces for model adapters} \label{ch5:AdapterInterfaces}

    Model adapters require a set of interfaces that specify the required behavior that a user
    must fulfill to make their model work properly within the framework. For that reason, the 
    library provides an interface for required explanation methods to perform a given 
    explanation that specifies behavior
    between a given explainer class and the user’s model adapter. This provides enough 
    granularity to the interface so that a user may freely decide which explanation method 
    they would like to provide support for.
    Interfaces for model adapters are implemented with the use of
    Python’s \mintinline{py}{ABC} module \cite{pythonABC}, which allows defining abstract 
    base classes for other classes. Every interface method is annotated 
    \mintinline{py}{@abstractmethod}, requiring a user to define it when performing inheritance, or it throws a \mintinline{py}{TypeError} exception otherwise.
    
\subsubsection{LIME method adapter}

    Adapter for LIME method (\ref{ch1:audioLIME}) defines the interface that the user's model needs to satisfy to be able to launch \texttt{audioLIME} explanation on it.
    This interface requires from user defining a single method \mintinline{py}{get_lime_predict_fn(self)}
    that must return a \mintinline{py}{np.array}.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, xleftmargin=12pt, linenos, mathescape]{py}
from abc import ABC, abstractmethod
import numpy as np

class LimeAdapter(ABC):
    """Abstract base class for LIME (Local Interpretable
       Model-agnostic Explanations) adapters"""
    @abstractmethod
    def get_lime_predict_fn(self) -> Callable[[np.ndarray], np.ndarray]: pass
    """Returns a function that takes an audio input and
       returns the model's prediction for that input."""
\end{minted}
\caption{Class definition of \texttt{LimeExplainer}.}
\label{fig:LimeAdapter}
\end{figure}

\subsubsection{SHAP method adapter}

    In the same way, adapter for SHAP method (\ref{ch1:ShapMethod}) defines the interface that the user's model needs to satisfy to be able to launch Shapley-Additive explanation on it.
    This interface requires from user defining a single method \mintinline{py}{get_shap_predict_fn(self)}
    that must return an \mintinline{py}{np.array}.

    Additionally, SHAP interface requires defining a \mintinline{py}{shap_prepare_inference_input} method that serves a purpose of additionally preprocessing data for a given model as shown in \ref{fig:ShapAdapter}.
    This is required as interface for explainers accept user's waveform input but they do expect an input which is the same as model's input which may be either waveform or a spectogram which requires additional conversion.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class ShapAdapter(ABC):
    """Abstract base class for SHAP (SHapley Additive exPlanations)
       adapters"""
    @abstractmethod
    def get_shap_predict_fn(self) -> Callable[[torch.Tensor], torch.Tensor]: pass
    """Returns a function that takes an audio input and returns
    the model's prediction for that input."""

    @abstractmethod
    def shap_prepare_inference_input(self, x: torch.Tensor) -> torch.Tensor: pass
    """Returns a function that takes an audio input
    and returns the model's prediction for that input."""
\end{minted}
\caption{Class definition of \texttt{ShapExplainer}.}
\label{fig:ShapAdapter}
\end{figure}

\subsubsection{LRP method adapter}

    The last method adapter for LRP method (\ref{ch1:LrpMethod}) also requires the user to define a single method \mintinline{py}{get_lime_predict_fn(self)} that must return \mintinline{py}{nn.Module}. This return type is required as Captum's LRP implementation takes it as an input.

    \begin{example}
        An exemplary implementation of LrpAdapter for an exemplary user's adapter \texttt{MyModelAdapter}
        is shown in \ref{fig:UserAdapterExample}.
    \end{example}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
from pylibxai.Interfaces import LrpAdapter

class MyModelAdapter(LrpAdapter):
    def __init__(self, model, device):
        self.model = model
        self.device = device

    def get_lrp_predict_fn(self):
        class MyModelWrapper(torch.nn.Module):
            def __init__(self, model):
                super(MyModelWrapper, self).__init__()
                self.predictor = predictor
                self.device = device

            def forward(self, x):
                self.predictor.model.eval()
                return self.predictor.model(x)

        return MyModelWrapper(self.predictor, self.device)
\end{minted}
\caption{A sample implementation of \texttt{LrpAdapter} for the exemplary \texttt{MyModelAdapter} class.}
\label{fig:UserAdapterExample}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, xleftmargin=12pt, linenos, mathescape]{py}
class LrpAdapter(ABC):
    """Abstract base class for LRP (Layer-wise Relevance Propagation)
       adapters"""
    @abstractmethod
    def get_lrp_predict_fn(self) -> nn.Module: pass
    """Returns a function that takes an audio input and
       returns the model's prediction for that input."""
\end{minted}
\caption{Class definition of \texttt{LrpAdapter}.}
\label{fig:LrpAdapter}
\end{figure}

\subsubsection{ModelLabelProvider interface}

The framework offers an additional interface for model adapters to pass their labels into the system.
The function requires the model to return a dictionary that maps a given class
name to its corresponding identifier, as shown in Fig. \ref{fig:ModelLabelProvider}.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, xleftmargin=12pt, linenos, mathescape]{py}
class ModelLabelProvider(ABC):
    """Abstract base class for providing labels for model predictions"""
    @abstractmethod
    def get_label_mapping(self) -> Dict[int, str]: pass
    """Returns a mapping from label IDs to label names."""

    @abstractmethod
    def map_target_to_id(self, target: str) -> int: pass
    """Maps a target label to its corresponding ID."""
\end{minted}
\caption{Class definition of \texttt{ModelLabelProvider}.}
\label{fig:ModelLabelProvider}
\end{figure}

\subsection{Model adapters} \label{ch5:ModelAdapters}

The framework requires users to provide their ML (machine learning) models
in an adapter class and includes a set of interfaces that define how these
adapters should be implemented (Sec. \ref{ch5:AdapterInterfaces}). These user-supplied classes adjust
model definition and wrap around logic responsible for model inference to allow
them to be used within the framework.

Adapters of users’ ML models serve as model classes in a Model-View-Controller,
where they are responsible for data processing. They are used by controller classes,
which are explainer classes in the case of the framework that pass the user’s input to them.
This way, the data manipulation is separated from the rest of the framework,
and new models may be integrated without modifying the rest of the codebase, which provides modularity.
The user may select individual interfaces that the framework’s API provides,
which allows them to implement only a subset of the functionality that a framework
offers and to skip unneeded parts. The user then may freely decide which interface
they would like to implement for their model between 
\texttt{LrpAdapter}, \texttt{LimeAdapter}, \texttt{ShapAdapter}, and \texttt{ModelLabelProvider}.

There are three adapters integrated into the library, which are HarmonicCNN, Cnn14, and GtzanCNN
(Sec. \ref{ch1:ModelSelection}). As their implementation is similar and long, HarmonicCNN is the model
that will described with a precision in the rest of this chapter.

%\subsubsection{HarmonicCNN adapter}
The given adapter first inherits the required interfaces to overload the required methods.
This is shown in Fig. \ref{fig:HarmonicCNNInit}, where the HarmonicCNN class inherits three classes:
\texttt{LimeAdapter}, \texttt{ShapAdapter}, and \texttt{ModelLabelProvider}:

The class initialization (Fig. \ref{fig:HarmonicCNNInit}) involves importing the model’s class definition
from the \textit{SotaMusicModels} repository \cite{Won2020-ej}. As the repository hosts a collection
of models and offers the retrieval of a given model through filling \mintinline{py}{config = Namespace()} with
configuration parameters, such as the model name specified as a \mintinline{py}{"hcnn"} string.
The HCNN (HermeticCNN) model is then retrieved \mintinline{py}{self.model = Predict.get_model(config)}.
In the last step, the model state is then loaded using \mintinline{py}{torch.load()} method.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class HarmonicCNN(LimeAdapter, ShapAdapter, ModelLabelProvider):
    def __init__(self, device='cuda'):
        # ...
        path_models = os.path.join(path_sota, 'models')

        # ...

        config = Namespace()
        config.dataset = "jamendo"  # we use the model trained on MSD
        config.model_type = "hcnn"
        config.model_load_path = os.path.join(path_models, config.dataset, config.model_type, 'best_model.pth')
        config.input_length = 5 * 16000
        config.batch_size = 1  # we analyze one chunk of the audio
        self.model = Predict.get_model(config)
        
        self.model_state = torch.load(config.model_load_path, map_location=self.device)
        self.model.cuda()
        self.config = config
    # ...
\end{minted}
\caption{Model initialization in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicCNNInit}
\end{figure}

To implement \texttt{ModelLabelProvider} interface, the class creates a mapping of model's class
labels stored in global variable \texttt{TAGS} and later uses them to overload
a method \mintinline{py}{get_label_mapping()} as shown in Fig. \ref{fig:HarmonicLabelMapping}.

\begin{figure}[h!]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
TAGS = ['genre---downtempo', 'genre---ambient', ...]

class HarmonicCNN(LimeAdapter, ShapAdapter, ModelLabelProvider):
    def __init__(self, device='cuda'):
        # ...
        self.label_to_id = {i: v for i, v in enumerate(TAGS)}
        self.id_to_label = {v: i for i, v in enumerate(TAGS)}
        # ...
    def get_label_mapping(self):
        """Returns the label mapping for the model."""
        return self.label_to_id
    # ...
\end{minted}
\caption{Set up of label classes in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicLabelMapping}
\end{figure}

The adapter defines a method \mintinline{py}{get_shap_predict_fn()} that implements
the \texttt{ShapAdapter} interface as shown in Fig. \ref{fig:HarmonicShapOverload}.
This method must return a function with a type \mintinline{py}{Callable[[torch.Tensor], torch.Tensor]}, % TODO: overflow hbox
that takes Pytorch \mintinline{py}{torch.Tensor} both as an argument and return type.
The callback then defines an inner function \mintinline{py}{predict_fn(x)} that contains
the standard inference code of HarmonicCNN, and then it returns that function.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def get_shap_predict_fn(self) -> Callable[[torch.Tensor], torch.Tensor]:
    # ...
    def predict_fn(x):
        # ... 
        output_dict = self.model(x)
        output_tensor = output_dict
        return output_tensor
    return predict_fn
\end{minted}
\caption{Supporting SHAP method in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicShapOverload}
\end{figure}

To implement \texttt{LrpAdapter} the adapter provides a definition for LRP
callback as shown in Fig. \ref{fig:HarmonicLrpOverload}. This method must return
a function object that returns a Python's \mintinline{py}{torch.nn.Module} meaning
that a user must create a wrapper class around it's model's implementation.
The function defines a \mintinline{py}{HarmonicCNNWrapper} that inherits from 
\mintinline{py}{torch.nn.Module} class and provides a required
\mintinline{py}{forward()} method where a HarmonicCNN inference is placed.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def get_lrp_predict_fn(self):
    class HarmonicCNNWrapper(torch.nn.Module):
        def __init__(self, model, device, model_state):
            super(SotaNNWrapper, self).__init__()
            self.model_state = model_state
            self.model = model 
            self.device = device
        def forward(self, x):
            #...

            output_dict = self.model(x)
            output_tensor = output_dict
            return output_tensor
    return HarmonicCNNWrapper(self.model, self.device, self.model_state)
\end{minted}
\caption{Support of LRP method in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicLrpOverload}
\end{figure}

The last implementation is a support for the LIME method in Fig. \ref{fig:HarmonicLLIMEOverload}.
This method must return a function with a type \mintinline{py}{Callable[[np.ndarray], np.ndarray]}
to fulfill the implementation of framework’s \texttt{LimeExplainer} which is based on \textit{audioLIME}
implementation mthat operates on numpy’s array types. The function then must
take a numpy \texttt{np.ndarray} as an input and also return it.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def get_lime_predict_fn(self) -> Callable[[np.ndarray], np.ndarray]:
    # ...

    def predict_fn(x_array):
        # convert numpy array into tensor
        audio = torch.zeros(len(x_array), self.config.input_length)
        for i in range(len(x_array)):
            audio[i] = torch.Tensor(x_array[i]).unsqueeze(0)
        audio = audio.cuda()
        audio = Variable(audio)
        output_dict = self.model(audio) # inference here (input is passed into model)
        output_tensor = output_dict.detach().cpu().numpy()
        return np.array(output_tensor)

    return predict_fn
\end{minted}
\caption{Support of LIME method in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicLLIMEOverload}
\end{figure}

%\subsubsection{CNN14 adapter} TODO: potrzebne?
%\subsubsection{GtzanCNN adapter} TODO: potrzebne?

\subsection{Explainer classes implementation}

Explanation methods are implemented as separate classes including \texttt{LimeAdapter}, \texttt{ShapExplainer}, and \texttt{LrpAdapter}.
The purpose of these classes is to implem TODO

These classes serves a purpose of controllers as part of Model-View-Controller
pattern so their purpose include managing k


\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}

\end{minted}
\caption{Support of LIME method in adapter \texttt{HarmonicCNN}.}
\label{fig:HarmonicLLIMEOverload}
\end{figure}


\subsubsection{LIME explainer}
\subsubsection{SHAP and LRP explainers}
\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class ShapExplainer:
    def __init__(self, model_adapter, context, device, view_type=None):

    def explain_instance(self, audio, target, background=None):
        pass
    
    def explain_instance_visualize(self, audio, target, type=None, background=None, attr_sign='positive'):  
        pass

    def get_attribution(self):
        pass

    def get_smoothed_attribution(self):
        pass
    
    def explain(self, audio, target):
        pass
\end{minted}
\caption{Class definition of \texttt{ShapExplainer}.}
\label{fig:ShapExplainer}
\end{figure}

\subsection{Context class definition}

The \texttt{PylibxaiContext} class ma`nages the data and encapsulates the data saving process.
It is an important bridge between model and view classes, formalizing how explanation results
are saved and accessed through a framework.
The class also implements the requirement \textit{Context Management}
and \textit{Ensure persistent result storage} as described
in Sec. (\ref{ch:NonFuncRequirements}), fully encapsulating the process of
saving the explanation results into the persistent storage. For the simplicity of design and also the purpose of serving
explanation result to the web frontend as discussed later in \ref{ch:ch5WebFrontend},
the implementation of this class saves all explanation results as files in a tree structure in a given working directory.

\begin{example}
    An exemplary implementation of the \texttt{write\_attribution} method that is defined in the
    \texttt{PylibxaiContext} class. The method saves an attribution of the explanation to a file in \texttt{JSON} format
    as shown in Fig. \ref{fig:WriteAttributionMethod}.
\end{example}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def write_attribution(self, smoothed_attribution, suffix):
    path = os.path.join(self.workdir, suffix)
    with open(path, 'w') as f:
        json.dump({
            "attributions": smoothed_attribution.tolist(),
        }, f, indent=4)
\end{minted}
\caption{The implementation of \texttt{write\_attribution} method of \texttt{PylibxaiContext} class.}
\label{fig:WriteAttributionMethod}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class PylibxaiContext:
    def __init__(self, workdir):
        self.workdir = workdir
        # [...]
    
    def write_plt_image(self, fig, suffix): pass # [...]
    
    def write_attribution(self, smoothed_attribution, suffix): pass # [...]

    def write_label_mapping(self, labels, suffix): pass # [...]
    
    def write_audio(self, audio, suffix, *args, **kwargs): pass # [...]
\end{minted}
\caption{Class definition of \texttt{PylibxaiContext}.}
\label{fig:PylibxaiContext}
\end{figure}

The \texttt{PylibxaiContext} class contains the following fields, as shown in its definition in Fig. \ref{fig:PylibxaiContext}:

\begin{itemize}
    \item \mintinline{py}{self.workdir} - the field holds a path to the working directory in which explanation results will be saved,
    \item \mintinline{py}{write_plt_image(self, fig, suffix)} - it handles saving the results of Matplotlib's (Sec. \ref{ch:ch3LabelSpec}) generated image,
    \item \mintinline{py}{write_attribution(self, smoothed_attribution, suffix)} - it handles saving explanation's attribution,
    \item \mintinline{py}{write_label_mapping(self, labels, suffix)} - it handles saving the mapping of labels into their indexes,
    \item \mintinline{py}{write_audio(self, audio, suffix, *args, **kwargs)} - it handles saving of an audio file,
\end{itemize}

\subsection{View classes}

The purpose of view classes is to present explanation results to the user in a readable form
according to Model-View-Controller pattern. The framework is implemented with modularity as a goal
and allows multiple views defined implementing \texttt{ViewInterface} interface.
The framework is implemented with modularity as a goal and allows multiple views
defined by implementing \texttt{ViewInterface} interface that defines how explanation
classes should be implemented in the framework.
This interface \texttt{ViewInterface} requires a user to define the necessary
methods required to initialize, start, and stop the given view as shown in Fig. \ref{fig:ViewType}.

\begin{itemize}
    \item \mintinline{py}{__init__(self, context)} - A given view is initialized with a provided context class.
    The view then may utilize the accumulated results of explanation in that class and present them to the user,

    \item \mintinline{py}{start(self)} - The start method is responsible for launching the given view. In the case of \texttt{WebView},
    it starts the backend and frontend servers. For \texttt{DebugView} it generates a terminal debugging logs showing based on 
    explanation results,
    
    \item \mintinline{py}{stop(self)} - The stop method is responsible for stopping and cleaning the execution of a given view class. It is required though it logic may be empty when there is nothing to stop as in case of \texttt{DebugView} class.
\end{itemize}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class ViewInterface(ABC):
    """Abstract base class for View Interface
    """
    @abstractmethod
    def __init__(self, context):
        self.context = context

    @abstractmethod
    def start(self) -> None: pass

    @abstractmethod
    def stop(self) -> None: pass    
\end{minted}
\caption{Class definition of \texttt{ViewInterface}.}
\label{fig:ViewInterface}
\end{figure}

Every view class defined in a framework must have an entry in enumeration \texttt{ViewType}.
For \texttt{WebView} there is an enumeration \mintinline{py}{DEBUG} provided, for \texttt{DebugView}
it is \mintinline{py}{DEBUG} as shown in Fig. \ref{fig:ViewType}.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class ViewType(IntEnum):
    """Enum-like class for View Types"""
    WEBVIEW = 1
    DEBUG = 2
    NONE = 2
\end{minted}
\caption{Class definition of \texttt{ViewType}.}
\label{fig:ViewType}
\end{figure}

\subsubsection{Web view}

The \texttt{WebView} implementation provides the main visualization of the explanation results
in the form of a web page. Its implementation only starts the underlying backend and frontend servers
in separate processes using Python's subprocess module.  The class implements the View interface to be later used within a framework. The definition of \texttt{WebView} is visible in Fig. \ref{fig:WebView}.

The whole startup process of the WebView contained in the \mintinline{py}{setup()} method contains the following steps:
\begin{enumerate}
    \item The startup procedure starts with the creation of a backend server using the function  \\
        \mintinline{py}{run_file_server(self.context.workdir, self.port)}.
        It takes the path of \\ the working directory from a given instance of the \texttt{Context} class
        and start a file serving server using Python's \mintinline{py}{socketserver.TCPServer} class.
        The implementation of file server is discussed later in Sec. \ref{ch5:FileServer},
    \item In the next step, the frontend server is initialized with the use of the Vite \cite{ViteDOC} tool,
        which it is based on. The \texttt{subprocess.Popen} argument takes the
        Vite command startup arguments as \mintinline{py}{['npm', 'run', 'dev']} that is run in the current working directory where the implementation of the frontend source code is placed, which is \mintinline{py}{cwd=self.vite_dir}. Additional parameters for the frontend web server are passed through the environmental variables, such as a port number that is passed to the frontend through the \mintinline{py}{'VITE_PYLIBXAI_STATIC_PORT'} environmental variable.
    
\end{enumerate}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class WebView(ViewInterface):
    def __init__(self, context, port=9000):
        super().__init__(context, port)
        self.vite_dir = get_install_path() / "pylibxai" / "pylibxai-ui"
        self.server = None
        self.vite_process = None

    def start(self):
        # Start the file server
        self.server = run_file_server(self.context.workdir, self.port)
        # Start the Vite UI
        env = os.environ.copy()
        env['VITE_PYLIBXAI_STATIC_PORT'] = str(self.port)
        print(f"Vite UI directory: {self.vite_dir}")
        self.vite_process = subprocess.Popen(
            ['npm', 'run', 'dev'],
            cwd=self.vite_dir,
            env=env,
            stdout=sys.stdout,
            stderr=sys.stderr,
            shell=True
        )
        print(f"Vite UI launched at http://localhost:{self.port}/ (UI dev server running)")

    def stop(self):
        if self.vite_process:
            self.vite_process.terminate()
            self.vite_process.wait()
            print("Vite UI process terminated.")
        if self.server:
            self.server.shutdown()
            print("File server stopped.")
\end{minted}
\caption{Class definition of \texttt{WebView}.}
\label{fig:WebView}
\end{figure}
 
\subsubsection{Debug View}

The purpose of the\texttt{DebugView} class is to enable quick debugging of the explanation
framework, displaying information about results to a terminal without launching any external windows or software.
It also shows an alternative view implementation to the \texttt{WebView} that shows the modularity of the explanation framework.

The implementation iterates over the content of the context class and displays its content in a structured manner to the user
using Python's \mintinline{py}{print()} function, as shown in the class implementation in Fig. \ref{fig:DebugView}. It does not contain any cleanup functionality, hence its \mintinline{py}{stop()} method has an empty body.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class DebugView(ViewInterface):
    def __init__(self, context):
        super().__init__(context)
        self.context = context

    def start(self):
        print("=== DEBUG VIEW: PylibxaiContext Content ===")
        print(f"Working directory: {self.context.workdir}")
        print()
        
        # Display directory structure
        print("Directory structure:")
        self._print_directory_tree(self.context.workdir)
        print()
        
        # Display content of each subdirectory
        subdirs = ["shap", "lrp", "lime"]
        for subdir in subdirs:
            subdir_path = os.path.join(self.context.workdir, subdir)
            if os.path.exists(subdir_path):
                print(f"=== {subdir.upper()} Directory Content ===")
                self._display_directory_content(subdir_path)
                print()
        
        # Display any JSON files in the root directory
        print("=== Root Directory Files ===")
        self._display_directory_content(self.context.workdir, root_only=True)
        print()
        
        print("=== DEBUG VIEW: Content Display Complete ===")

    def stop(self):
        pass
\end{minted}
\caption{Class definition of \texttt{DebugView}.}
\label{fig:DebugView}
\end{figure}

\subsection{Backend Server implementation} \label{ch5:FileServer}

The backend server implementation is implemented using Python's built-in \texttt{TCPServer} class from
\texttt{socketserver} module. It allows serving files placed in a given directory provided as an input.
This allows serving explanation results that are contained in an instance of \texttt{PylibxaiContext} that
are passed to the view class from the framework.

Initially, the system path is changed into the working directory as shown in Fig. \ref{fig:BackendServer},
so that the server is launched serving the files inside, creating endpoints for each stored resource.
The advantage of such an implementation is automatic scalability with the content of a working directory,
which provides simplicity of the implementation. The server is launched in a separate thread using the 
\mintinline{py}{threading.Thread} module over a given port provided as an input
\mintinline{py}{socketserver.TCPServer(("", port), handler)}.

Additionally, the server sets up a minimal CORS access control rules by defining a custom
class \texttt{CORSHTTPRequestHandler} as shown in Fig \ref{fig:CORSHTTPRequestHandler}.

\begin{example}
    An exemplary explanation result which is explanation's attribution saved as a file \texttt{workdir/lrp/attribution.json}
    will have an HTTP endpoint generated by the backend server of \texttt{/lrp/attribution.json}.
\end{example}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
def run_file_server(directory, port=9000):
    """Start a file server in a background thread."""
    os.chdir(directory)

    handler = CORSHTTPRequestHandler
    httpd = socketserver.TCPServer(("", port), handler)

    print(f"Serving files from {directory} at http://localhost:{port}/")

    # Run in background thread
    thread = threading.Thread(target=httpd.serve_forever, daemon=True)
    thread.start()

    return httpd  # To stop later with httpd.shutdown()

\end{minted}
\caption{Implementation of backend server.}
\label{fig:BackendServer}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
class CORSHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def end_headers(self):
        # Add CORS headers
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'X-Requested-With')

        
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        return super().end_headers()

    def do_OPTIONS(self):
        # Handle preflight requests
        self.send_response(200)
        self.end_headers()
\end{minted}
\caption{Class definition of \texttt{CORSHTTPRequestHandler}.}
\label{fig:CORSHTTPRequestHandler}
\end{figure}

\subsection{Web frontend implementation} \label{ch:ch5WebFrontend}

The main visualization of explanation results is created in the form of a web page displaying different results of
an explanation, such as audio waveform visualization, explanation’s attribution, or spectrograms graphically in a
browser. The frontend implementation is built using the React JavaScript library \cite{ReactDOC}
and Bootstrap CSS framework \cite{BootstrapDOC} and follows a component-based architecture where each interface
element is encapsulated as a reusable React component

%\subsubsection{Frontend layout organization}

The main objective of web page design is to offer a separate display of each explanation method to not
confusing the results with each other, as discussed in Sec. \ref{ch:ch3LabelSpec}.
The Web page’s layout is organized to emphasize the separation of different explanation methods to allow
a user to quickly switch between them. For that, the web page organizes different data displayed in
separate sections and provides a sidebar to switch between them.

The organization of the website's layout is shown in Fig. \ref{fig:PageLayout} and includes:

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{page_layout.png}
    \caption{The organization of website. Each components is identified by a number.}
    \label{fig:PageLayout}
\end{figure}

\begin{itemize}
    \item \textbf{Sidebar (1)} - Sidebar allows users to navigate between different sections, including mode
        information, SHAP, LIME, and LRP explanations, using a list of buttons, highlighting
        the actively shown tab,
    \item \textbf{Navigation bar (2)} - The Navigation bar is a permanently visible page element that is sticky
        to its upper side. It contains various control icons on its left side, including a notification
        bell icon or a help icon,
    \item \textbf{Notification panel and help (3)} - these utilities elements are provided
        to a user to show them helper information such as framework's versioning or any errors
        and notifications that were encountered through the process of explanation visualization.
        The exemplary content of notification panel is discussed in the Sec.\ref{ch4:NotificationPanel}. 
    \item \textbf{Main content area (4)} - Main content area display currently selected section
        where the data displayed is organized vertically in a folded sections, allowing a user
        to hide unwanted information. Displayed information include model information, audio playback,
        attribution visualization, or explanation results.
\end{itemize}

\subsubsection{Frontend component overview}

The implementation consists of multiple components,
each defined in a separate file, including:

\begin{itemize}[itemsep=1\baselineskip]
	\item \textbf{App} - Main application component that manages global state,
        including managing the notification queue or the currently selected page to render,
    \item \textbf{Navbar} - Fixed navigation bar displaying supporting components,
        including notification bell, or version details,
    \item \textbf{ContentPage} - Manages rendering of the main content area and
        routing between different result sections of the interface,
    \item \textbf{ModelInfo} - Component that displays model information,
        such a label mappings, presenting their names along with a class identifier,
    \item \textbf{Lime, Shap, Lrp} - Each of these components manages the display of its
        respective explanation method’s result. This includes displaying audio playback,
        attribution visualizations, or interactive charts,
    \item \textbf{Footer} - Provides footer of the web page.
\end{itemize}

Frontend retrieves explanation results data from the backend server that was set up by the
\texttt{WebView} class to serve collected data through the process of explanation.
Both servers are set up by the view class and operate on localhost. 
Frontend retrieves the backend server's port from environmental variables using the \mintinline{js}{meta.env}
module:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
import.meta.env.VITE_PYLIBXAI_STATIC_PORT
\end{minted}

Based on that, we define a URL of the backend server from which all frontends’ requests
will be made, which is stored in a variable:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
staticBaseUrl = `http://localhost:\${staticPort}`.
\end{minted}
The requests are done using JavaScript's built-in Fetch API utilizing \mintinline{js}{fetch()} method.
An example of data fetching from backend which is fetching of explanation's attribution
contained in a file in a JSON format as seen in Fig. \ref{fig:ShapComponentDataFetching}.

An instance of an HTTP request is generated using the \mintinline{js}{fetch()} method. After that, the data is processed further by the frontend to create a visualization of it
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
const response = await fetch(`\${staticBaseUrl}/shap/shap_attributions.json`)
\end{minted}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{js}
function Shap() {
  // [...]

  // Get the static file server port from environment variables
  const staticPort = import.meta.env.VITE_PYLIBXAI_STATIC_PORT || '9000'
  const staticBaseUrl = `http://localhost:${staticPort}`

  // [...]

  useEffect(() => {
    // Fetch SHAP attributions data
    const fetchAttributions = async () => {
      try {
        setIsLoading(true)
        const response = await fetch(`${staticBaseUrl}/shap/shap_attributions.json`)
        if (!response.ok) {
          throw new Error(`HTTP error! Status: ${response.status}`)
        }
        const data = await response.json()
        if (!data.attributions || !Array.isArray(data.attributions)) {
          throw new Error('Data is not in the expected format (object with attributions array)')
        }
        setAttributions(data.attributions)
        setIsLoading(false)
      } catch (error) {
        setIsLoading(false)
        pushNotification({ type: 'error', message: `SHAP: ${error.message}` })
      }
    }
    fetchAttributions()
  }, [staticBaseUrl, pushNotification])
}
\end{minted}
\caption{Data fetching logic defined in a \texttt{Shap} component.}
\label{fig:ShapComponentDataFetching}
\end{figure}
    
\subsubsection{Visualization of audio waveform}

The frontend provides visualization and audio playback functionality of
different audio pieces that appear through the process of explanation:

\begin{itemize}
    \item Original input - it is visualized so that a user access and listen
          to the original input,
    \item Lime method explanation - the LIME method provides a result in a form of an audio
          where an explanation which is the part of input that was the most important for
          the decision of explanation is preserved in the audio and the rest is muted.
\end{itemize}

The visualization and playback of the audio pieces is implemented using the Wavesurfer.js
library \cite{WavesurferDOC}. It is used in the \texttt{Lime}, \texttt{Shap}, and \texttt{Lrp}
components to visualize the input audio, and the LIME method component also uses it for the
visualization of its explanation. The advantage of the Wavesurfer.js library is that it
provides interactive playback functionality meaning that a user may listen to the original
audio and select any moment of the audio to listen to.

The exemplary initialization of WaveSurfer's audio playback in the \texttt{Shap} component is shown
in Fig. \ref{fig:ShapAudioPlayback}. The use is analogous for the \texttt{Lime} and \texttt{Lrp} components.
An audio playback defined in this way will generate the following view as shown Fig. \ref{fig:AudioPlayback}.

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{js}
function Shap() {
  // [...]

  wavesurfer.current = WaveSurfer.create({
    container: waveformRef.current,
    waveColor: '#4F4A85',
    progressColor: '#383351',
    cursorColor: '#383351',
    barWidth: 2,
    barRadius: 3,
    responsive: true,
    height: 100,
    barGap: 3
  })

  // [...]
}
\end{minted}
\caption{Audio playback definition in a \texttt{Shap} component.}
\label{fig:ShapAudioPlayback}
\end{figure}

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{audio_playback.png}
    \caption{The visualization of audio playback.}
    \label{fig:AudioPlayback}
\end{figure}

%\subsubsection{Visualization of attribution intensity}

The frontend must also provide a visualization of attribution intensity (Sec. \ref{ch2:AttrIntensity}) over time.
This functionality is implemented using ChartJS library \cite{ChartJsDoc}.
The attribution intensity is a summation of all waveform frequencies over a time so it may
be displayed using a simple linear plot that the ChartJS library provides.
The exemplary attribution plot definition is shown in Fig. \ref{fig:ShapAttrPlot}
and the generated plot corresponding to it is shown in Fig. \ref{fig:AttrPlot}.

\begin{figure}[h!] % TODO: h! musi być wyłączone
    \centering
    \includegraphics[width=15cm]{attribution_plot.png}
    \caption{The linear plot displaying attribution during time of an audio.}
    \label{fig:AttrPlot}
\end{figure}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{js}
    chartInstance.current = new Chart(ctx, {
      type: 'line',
      data: {
        labels: labels,
        datasets: [{ label: `SHAP Attribution Values`, data: attributions,
          fill: false, borderColor: 'rgb(42, 123, 198)', tension: 0.1
        }]
      },
      options: {
        responsive: true,
        maintainAspectRatio: false,
        plugins: {
          title: { display: true, text: 'SHAP Attributions' },
          tooltip: {
            callbacks: {
              title: (tooltipItems) => { return `Frame: ${tooltipItems[0].label}` },
              label: (tooltipItem) => { return `Value: ${tooltipItem.raw.toFixed(4)}` }
            }
          }
        },
        scales: {
          x: { title: { display: true, text: 'Frame Index' } },
          y: { beginAtZero: true, title: { display: true, text: 'Attribution Value' }
          }
        }
      }
    })
\end{minted}
\caption{Definition of attribution plot defined in the \texttt{Shap} component.}
\label{fig:ShapAttrPlot}
\end{figure}

\subsubsection{Error and Notification system}

The notification panel is responsible for displaying to the user any notifications
and errors that appear during the execution of the frontend. It is implemented as
a fixed-size bounded queue of size 10, meaning than any new errors that would exceed
the maximal limit of the queue.

The logic of pushing any new notification into the queue is implemented in the function
\mintinline{js}{pushNotification()} as shown in Fig. \ref{fig:NotificationImpl}.
The function adds new notification at the start of the array:

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
const next = [
  { id: Date.now() + Math.random(), ...notification },
    ...prev
  ];
\end{minted}

It then discards the oldest notifications when the limit exceeds the maximum size:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
next.slice(0, maxNotifications);
\end{minted}

\begin{figure}%[h]
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{js}
// Notification context
export const NotificationContext = createContext({
  notifications: [],
  pushNotification: () => {},
});

function App() {
  const [notifications, setNotifications] = useState([]); // {id, type, message}
  const maxNotifications = 10;
  const handleNavigation = (section) => {
    setCurrentSection(section);
  };

  // Push notification (circular queue)
  const pushNotification = useCallback((notification) => {
    setNotifications(prev => {
      const next = [
        { id: Date.now() + Math.random(), ...notification },
        ...prev
      ];
      return next.slice(0, maxNotifications);
    });
  }, []);
}
\end{minted}
\caption{The implementation of ring-buffer responsible for storing notifications.}
\label{fig:NotificationImpl}
\end{figure}

Later, the notification queue may be used across the frontend code to handle different
events that have occurred. The user provides a type of event into the \mintinline{js}{'type'} key and
the message into the \mintinline{js}{'message'} key:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{js}
pushNotification({ type: 'error', message: `SHAP: Failed to load audio: ${error.message}` })
\end{minted}

\clearpage % TODO: usuń
\subsection{Explanation runner command-line script}

The framework provides a command-line tool script as a helper tool for quick execution
of explanation methods on the user’s input using models integrated into the library
(Sec. \ref{ch5:ModelAdapters}). The full script implementation
is shown in Appendix. \ref{appendix:ScriptListing}.

For this, the script sets up terminal options as described in Sec. \ref{ch4:CmdTool}, with a use of Python's \texttt{argparse.ArgumentParser} class:
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
parser = argparse.ArgumentParser(description="Process a model name and input path.")
    
parser.add_argument('-m', '--model', type=str, required=True,
                    help="Name of the model to use [{sota_music, paans, gtzan},]...")
# [...]
args = parser.parse_args()
\end{minted}

Then, to launch a given explanation method, an instance of the \texttt{PylibxaiContext} class
is set up with a directory provided by the user through command-line options:

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
context = PylibxaiContext(args.workdir)
\end{minted}

The script then initializes the appropriate model adapter provided by the user through the \texttt{-m/--model} parameter: 
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
if args.model == "HCNN":
    adapter = HarmonicCNN(device=device)
elif args.model == "CNN14":
    adapter = Cnn14Adapter(device=device)
elif args.model == "GtzanCNN":
    adapter = GtzanCNNAdapter(model_path=GTZAN_MODEL_PATH, device=device)
else:
    print('Invalid value for -m/--model argument, available: [HCNN, CNN14, GtzanCNN].')
    return
\end{minted}

The script then sets up the web page view if it was selected or defaults to debug view otherwise: 
\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
view_type = ViewType.WEBVIEW if args.visualize else ViewType.DEBUG
\end{minted}

Then, the initial files are saved into the current context, which includes original audio input
and a mapping of class labels into their identifiers. The mapping is performed conditionally,
whether the model adapter implements \texttt{ModelLabelProvider}.

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
context.write_audio(args.input, os.path.join("input.wav"))
if issubclass(type(adapter), ModelLabelProvider):
    context.write_label_mapping(adapter.get_label_mapping(), os.path.join("labels.json"))
\end{minted}

Finally, after setting up required classes and parsing all command-line options,
the script invokes selected explanation methods by running their
explainer class on the user’s input:

\begin{minted}[numbersep=12pt, fontsize=\footnotesize, breaklines, xleftmargin=12pt, mathescape]{py}
if "lime" in expls:
    view = view_type if expl_count == 1 else ViewType.NONE
    expl_count -= 1
    explainer = LimeExplainer(adapter, context, view_type=view)
    explainer.explain(args.input, target=None)
if "lrp" in expls:
    # [...]
if "shap" in expls:
    # [...]
\end{minted}

% #############################################
%
% Rozdział 6 - Verification
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Verification} \label{ch:verification}

\subsection{Case study: integrating Simple GTZAN model into framework}
\subsection{Unit test organization}
\subsection{Model tests, adapter test, backend tests}

% #############################################
%
% Rozdział 7 - Podsumowanie
%
% #############################################
\clearpage % Rozdziały zaczynamy od nowej strony.
\section{Summary} \label{ch:summary}

%---------------
% Bibliografia
%---------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\printbibliography
\clearpage

% Wykaz symboli i skrótów.
% Pamiętaj, żeby posortować symbole alfabetycznie
% we własnym zakresie. Makro \acronymlist
% generuje właściwy tytuł sekcji, w zależności od języka.
% Makro \acronym dodaje skrót/symbol do listy,
% zapewniając podstawowe formatowanie.
\acronymlist
\acronym{EiTI}{Wydział Elektroniki i Technik Informacyjnych}
\acronym{PW}{Politechnika Warszawska}
\acronym{WEIRD}{ang. \emph{Western, Educated, Industrialized, Rich and Democratic}}
\acronym{SOTA}{State-of-art Music Models}
\acronym{LRP}{Layerwise propagation}
\acronym{JSON}{JavaScript Object Notation}
\acronym{CORS}{Cross-origin resource sharing}
\acronym{HTTP}{Hypertext Transfer Protocol}
\vspace{0.8cm}

%--------------------------------------
% Spisy: rysunków, tabel, załączników
%--------------------------------------
\pagestyle{plain}

\listoffigurestoc    % Spis rysunków.
\vspace{1cm}         % vertical space
\listoftablestoc     % Spis tabel.
\vspace{1cm}         % vertical space
\listofappendicestoc % Spis załączników

%-------------
% Załączniki
%-------------

% Obrazki i tabele w załącznikach nie trafiają do spisów
\captionsetup[figure]{list=no}
\captionsetup[table]{list=no}

% Załącznik 1
\clearpage

\appendix{Listing of the script \texttt{pylibxai\_explain.py}} \label{appendix:ScriptListing}
\begin{minted}[numbersep=12pt, fontsize=\scriptsize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
import torch
import argparse
import torchaudio
import os

from pylibxai.model_adapters import HarmonicCNN, Cnn14Adapter, GtzanCNNAdapter
from pylibxai.pylibxai_context import PylibxaiContext
from pylibxai.Explainers import LimeExplainer, ShapExplainer, LRPExplainer
from pylibxai.Interfaces import ViewType, ModelLabelProvider
from utils import get_install_path

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
GTZAN_MODEL_PATH= get_install_path() / "pylibxai" / "models" / "GtzanCNN" / "gtzan_cnn.ckpt"

def main():
    parser = argparse.ArgumentParser(description="Process a model name and input path.")
    
    parser.add_argument('-m', '--model', type=str, required=True,
                        help="Name of the model to use [{sota_music, paans, gtzan},]...")
    parser.add_argument('-u', '--visualize', action='store_true',
                        help="Enable visualization of audio in browser-based UI.")
    parser.add_argument('-e', '--explainer', type=str, required=True,
                        help="Name of the explainer to use [lime, shap, lrp].")
    parser.add_argument('-t', '--target', type=str, required=True,
                        help="Name or index of the label to explain.\
                              Mapping is done automatically based on the model if the model provides it.") 
    parser.add_argument('-i', '--input', type=str, required=True,
                        help="Path to the input file or directory.") 
    parser.add_argument('-w', '--workdir', type=str, required=True,
                        help="Path to the workdir directory.")
    parser.add_argument('-d', '--device', type=str, default=DEVICE,
                        help="Device to use for computation [cpu, cuda]. Default is 'cuda' if available, otherwise 'cpu'.")
    args = parser.parse_args()
    
    device = args.device if args.device is not None else DEVICE
    assert device in ['cpu', 'cuda'], "Device must be either 'cpu' or 'cuda'."
    
    expls = args.explainer.split(",")
    assert all(ex in ["lime", "shap", "lrp"] for ex in expls), \
        "Invalid explainer specified. Available options: [lime, shap, lrp]."

    context = PylibxaiContext(args.workdir)

    if args.model == "HCNN":
        adapter = HarmonicCNN(device=device)
    elif args.model == "CNN14":
        adapter = Cnn14Adapter(device=device)
    elif args.model == "GtzanCNN":
        adapter = GtzanCNNAdapter(model_path=GTZAN_MODEL_PATH, device=device)
    else:
        print('Invalid value for -m/--model argument, available: [HCNN, CNN14, GtzanCNN].')
        return
    
    view_type = ViewType.WEBVIEW if args.visualize else ViewType.DEBUG
    expl_count = len(expls)

    # Attempt parsing label as an integer, if it fails then assume it's a string label
    try:
\end{minted}
\begin{minted}[numbersep=12pt, firstnumber=59, fontsize=\scriptsize, breaklines, xleftmargin=12pt, linenos, mathescape]{py}
        target = int(args.target)
    except ValueError:
        target = args.target

    # copy input audio to workdir
    context.write_audio(args.input, os.path.join("input.wav"))
    if issubclass(type(adapter), ModelLabelProvider):
        context.write_label_mapping(adapter.get_label_mapping(), os.path.join("labels.json"))
    
    if "lime" in expls:
        view = view_type if expl_count == 1 else ViewType.NONE
        expl_count -= 1
        explainer = LimeExplainer(adapter, context, view_type=view)
        explainer.explain(args.input, target=None)
    if "lrp" in expls:
        view = view_type if expl_count == 1 else ViewType.NONE
        expl_count -= 1
        audio, _ = torchaudio.load(args.input, normalize=True)
        audio = audio.to(device)
        explainer = LRPExplainer(adapter, context, device, view_type=view)
    if "shap" in expls:
        view = view_type if expl_count == 1 else ViewType.NONE
        expl_count -= 1
        audio, _ = torchaudio.load(args.input, normalize=True)
        audio = audio.to(device)
        explainer = ShapExplainer(adapter, context, device, view_type=view)
        explainer.explain(audio, target=target)

if __name__ == '__main__':
    main()
\end{minted}

% Załącznik 2
\clearpage
\appendix{Nazwa załącznika 2}
\lipsum[1-2]
\begin{table}[!h] \centering
    \caption{Tabela w załączniku.}
    \begin{tabular} {| c | c | r |} \hline
        Kolumna 1       & Kolumna 2 & Liczba \\ \hline\hline
        cell1           & cell2     & 60     \\ \hline
        \multicolumn{2}{|r|}{Suma:} & 123,45 \\ \hline
    \end{tabular}
\end{table}
\lipsum[3-4]

% Używając powyższych spisów jako szablonu,
% możesz dodać również swój własny wykaz,
% np. spis algorytmów.

\end{document} % Dobranoc.
